{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUbVwthVRbP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5685f9a5-4934-45b3-b73a-9a57e72b41c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  9 02:04:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s6_Xl13BRfYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "mRqllQUb_Q9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "miXR42YhPPJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/archive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faClh8sFRgZG",
        "outputId": "e3db4ef8-678d-46b2-f9a8-c1311c07e5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " best_model_1028.52485385624.pth        best_model_2d_test.pth\n",
            " best_model_1036.3455625417614.pth      best_model_429.92960906408854.pth\n",
            " best_model_1045.4423704824671.pth      best_model.pth\n",
            " best_model_1046.6953950409832.pth      cubes444_Santa_states_and_n_best_moves.csv\n",
            " best_model_1048.3806560596204.pth      empty.txt\n",
            " best_model_1049.3992924525917.pth     'Heis 3 gens data'\n",
            " best_model_1049.4389572528742.pth      HeisenbergComputations.sage\n",
            " best_model_1056.1936967242325.pth      puzzle_info.csv\n",
            " best_model_1066.2490914415823.pth      puzzles_best_scores_numeric_states_etc.csv\n",
            " best_model_1069.0997380974331.pth     'Rationality of growths of groups Ho.pdf'\n",
            " best_model_1081.9982093330839.pth      UT4_growth\n",
            " best_model_1082.7521646249431.pth      UT4_states_and_distances\n",
            " best_model_1084.7952571798976.pth      UT5_growth\n",
            " best_model_1086.6350008167417.pth      UT5_states_and_distances\n",
            " best_model_1088.3257784321752.pth      UT6_growth\n",
            " best_model_1088.901281108623.pth       UT6_states_and_distances\n",
            " best_model_2d_1121.4335134104028.pth   UT7_growth\n",
            " best_model_2d_1331.815677937454.pth    UT8_growth\n",
            " best_model_2d_1517.3123442455742.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNKh_Ug1RrJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bzw92izE3rLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "8jY5w-qL3xow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    dtype = torch.float\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    dtype = torch.float#int16\n",
        "    dtype = torch.int16\n",
        "\n",
        "print(dtype)\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb-nVGFn31l_",
        "outputId": "633bbd93-e2fc-4024-affa-aa6e57668aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dimension = 10\n",
        "# dic={5: 20,\n",
        "#      10: 140,\n",
        "#     20:\t250,\n",
        "# 21:\t273,\n",
        "# 22:\t297,\n",
        "# 23:\t322,\n",
        "# 24:\t348,\n",
        "# 25:\t375,\n",
        "# 26:\t403,\n",
        "# 27:\t432,\n",
        "# 28:\t462,\n",
        "# 29:\t493,\n",
        "# 30:\t525,\n",
        "# 31:\t558,\n",
        "# 32:\t592,\n",
        "# 35:\t700,\n",
        "# 37:\t777,\n",
        "# 40:\t900,\n",
        "# 45:\t1125,\n",
        "# 50:\t1375,\n",
        "# 60:\t1950,\n",
        "# 70:\t2625,\n",
        "# 80:\t3400,\n",
        "# 90:\t4275,\n",
        "# 100: 5250,\n",
        "# 150:\t11625,\n",
        "# 200:\t20500,\n",
        "# 300:\t45750}\n",
        "# n_random_walk_length = dic[dimension]\n",
        "# rate_of_diffusion_distance =1\n",
        "# n_random_walk_length = n_random_walk_length*rate_of_diffusion_distance\n",
        "# n_random_walks_to_generate_validation_fixed = 100\n",
        "# n_random_walk_length = 20\n",
        "# n_random_walks_to_generate = 1000\n",
        "\n",
        "# def make_S_N(N):\n",
        "#     return [\n",
        "#         [1,0,]+[q+2 for q in range(N-2)]       ,\n",
        "#                [q+1 for q in range(N-1)] + [0,],\n",
        "#         [N-1,]+[q+0 for q in range(N-1)]       ,\n",
        "#     ]\n",
        "# list_generators = make_S_N(dimension)\n"
      ],
      "metadata": {
        "id": "rkQA5Fea4RTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_walks(n_random_walk_length,  n_random_walks_to_generate):\n",
        "      '''\n",
        "      Output:\n",
        "      returns X,y: X - array of states, y - number of steps rw achieves it\n",
        "\n",
        "      Input:\n",
        "      generators - generators (moves) to make random walks  (permutations),\n",
        "          can be list of vectors or array with vstacked vectors\n",
        "      n_random_walk_length - number of visited nodes, i.e. number of steps + 1\n",
        "      n_random_walks_to_generate - how many random walks will run in parrallel\n",
        "      rw_start - initial states for random walks - by default we will use 0,1,2,3 ...\n",
        "          Can be vector or array\n",
        "          If it is vector it will be broadcasted n_random_walks_to_generate times,\n",
        "          If it is array n_random_walks_to_generate - input n_random_walks_to_generate will be ignored\n",
        "              and will be assigned: n_random_walks_to_generate = rw_start.shape[0]\n",
        "\n",
        "      '''\n",
        "      state_size        = len(list_generators[0])\n",
        "      n_generators      = len(list_generators  )\n",
        "      dtype_generators = torch.int64\n",
        "      tensor_generators = torch.tensor(   list_generators       , device = device, dtype = dtype_generators, )\n",
        "      state_destination = torch.arange( state_size, device=device, dtype = dtype).reshape(-1,state_size)\n",
        "      array_of_states = state_destination.to(device).view( 1, state_size  ).expand( n_random_walks_to_generate, state_size )\n",
        "\n",
        "      # Output: X,y - states, y - how many steps we achieve them\n",
        "      # Allocate memory:\n",
        "      X = torch.zeros( n_random_walks_to_generate*n_random_walk_length , state_size, device=device, dtype = dtype )\n",
        "\n",
        "      # First portion of data  - just our state_rw_start state  multiplexed many times\n",
        "      X[:n_random_walks_to_generate,:] = array_of_states\n",
        "      y        = torch.tensor(range(n_random_walks_to_generate*n_random_walk_length), device=device)//n_random_walks_to_generate\n",
        "      IX_moves = torch.randint(0, n_generators, (n_random_walks_to_generate*n_random_walk_length-n_random_walks_to_generate,), dtype = torch.int32, device=device)\n",
        "\n",
        "      # Technical to make array[ IX_array] we need  actually to write array[ range(N), IX_array  ]\n",
        "\n",
        "      # Main loop\n",
        "\n",
        "      argnr = (torch.arange(n_random_walks_to_generate, dtype=torch.long, device=device)*torch.ones((n_random_walks_to_generate,tensor_generators.shape[1]), dtype=torch.long, device=device).T).T\n",
        "\n",
        "      for i_step in range(1,n_random_walk_length):\n",
        "          a,b,c = (i_step-1)*n_random_walks_to_generate, (i_step-0)*n_random_walks_to_generate, (i_step+1)*n_random_walks_to_generate\n",
        "          #X[ b:c, : ] = X[ a:b, : ][argnr, self.tensor_generators[IX_moves[ a:b ],:]]\n",
        "          X[ b:c, : ] = torch.gather( X[ a:b, : ], 1, tensor_generators[IX_moves[ a:b ],:] )\n",
        "\n",
        "      return X,y"
      ],
      "metadata": {
        "id": "w7Ius8T-4kg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcX70AnUUqUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test, y_test = random_walks(n_random_walk_length       = n_random_walk_length,\n",
        "#                                                      n_random_walks_to_generate = n_random_walks_to_generate)\n",
        "# test_dataset = MyIntSequenceDataset(X_test, y_test)\n",
        "# # train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "herSa6BN9UIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = 1\n",
        "# for j in range(4):\n",
        "#   print(j, X_test[i + j * 500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K36b9qdp_r1b",
        "outputId": "72a69b4b-4b6c-4d09-912b-9e2b43e42ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], device='cuda:0')\n",
            "1 tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], device='cuda:0')\n",
            "2 tensor([1., 2., 3., 4., 5., 6., 7., 8., 9., 0.], device='cuda:0')\n",
            "3 tensor([9., 0., 1., 2., 3., 4., 5., 6., 7., 8.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = len(list_generators[0])\n",
        "hash_vec = torch.randint(-(2**60), 2**60, (state_size,), dtype = torch.int64, device = device )\n",
        "# hash_vec = torch.randint(0, 10**9, (state_size,), dtype = torch.int64, device = device )\n",
        "\n",
        "def get_unique_states(states: torch.Tensor) -> torch.Tensor:\n",
        "    '''\n",
        "    Return matrix with unique rows for input matrix \"states\"\n",
        "    I.e. duplicate rows are dropped.\n",
        "    For fast implementation: we use hashing via scalar/dot product.\n",
        "    Note: output order of rows is different from the original.\n",
        "    '''\n",
        "    # Note: that implementation is 30 times faster than torch.unique(states, dim = 0) - because we use hashes  (see K.Khoruzhii: https://t.me/sberlogasci/10989/15920)\n",
        "    # Note: torch.unique does not support returning of indices of unique element so we cannot use it\n",
        "    # That is in contrast to numpy.unique which supports - set: return_index = True\n",
        "\n",
        "\n",
        "    # Hashing rows of states matrix:\n",
        "    hashed = torch.sum(hash_vec * states, dim=1) # Compute hashes.\n",
        "        # It is same as matrix product torch.matmul(hash_vec , states )\n",
        "        # but pay attention: such code work with GPU for integers\n",
        "        # While torch.matmul - does not work for GPU for integer data types,\n",
        "        # since old GPU hardware (before 2020: P100, T4) does not support integer matrix multiplication\n",
        "\n",
        "    # Sort\n",
        "    hashed_sorted, idx = torch.sort(hashed)\n",
        "    # Mask selects elements which are different from the consequite - that is unique elements (since vector is sorted on the previous step)\n",
        "    mask = torch.concat((torch.tensor([True], device = device), hashed_sorted[1:] - hashed_sorted[:-1] > 0))\n",
        "    return states[idx][mask]\n",
        "\n",
        "\n",
        "class MatrixDatasetRegrSmall(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = torch.tensor(X, dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = F.one_hot(self.X[idx], num_classes=self.X.shape[1]).float()\n",
        "        return x.unsqueeze(0)\n",
        "\n",
        "class MyIntSequenceDatasetSmall(Dataset):\n",
        "    def __init__(self, X, is_regression=True):\n",
        "        # Store integer inputs and targets\n",
        "        dtype_y = torch.float32 if is_regression else torch.long\n",
        "        self.X = torch.tensor(X, dtype=torch.long)  # shape (N, 96)\n",
        "\n",
        "        self.is_regression = is_regression\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return (input_sequence, target)\n",
        "        return self.X[idx]\n",
        "\n",
        "def make_predictions_on_array(model, array):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    dataset = MyIntSequenceDatasetSmall(array)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for batch_X in loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "\n",
        "    return all_preds\n",
        "\n"
      ],
      "metadata": {
        "id": "OMNVZ4UHK9HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktSubaaJS9Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def beam_search_func():\n",
        "\n",
        "#   n_scrambles_starting_state = 1000\n",
        "#   n_gens = len(list_generators)\n",
        "#   print('n_scrambles_starting_state:',n_scrambles_starting_state)\n",
        "\n",
        "#   state_size = len(list_generators[0])\n",
        "#   # destination state - \"solved puzzle state\"\n",
        "#   state_destination = torch.arange( state_size, device=device, dtype = dtype)\n",
        "\n",
        "\n",
        "#   # Scramble - generate state which will be start of beam search\n",
        "#   state_start = state_destination\n",
        "#   for k in range(n_scrambles_starting_state):\n",
        "#       IX_move = np.random.randint(0, n_gens, dtype = int) # random moves indixes\n",
        "#       state_start = state_start[ list_generators[IX_move]] # all_moves[IX_moves,:] ]\n",
        "\n",
        "\n",
        "#   beam_width = 10_000\n",
        "\n",
        "#   # Iabsolutenitialize array of states\n",
        "#   array_of_states = state_start.view(1, state_size  ).clone().to(dtype).to(device)\n",
        "\n",
        "#   i_step_max = 400\n",
        "#   for i_step in range(1,i_step_max+1):\n",
        "\n",
        "#       # Technical preparation: to apply permutations to array we need the trick:\n",
        "#       # naively it is: array[ :, IX_array], but actually we need  to write array[ range(N)[:, np.newaxis], IX_array  ]\n",
        "#       row_indices = np.arange( array_of_states.shape[0] )[:, np.newaxis]\n",
        "\n",
        "#       # Apply all moves to all current states, all new states saved in array_of_states_new\n",
        "#       array_of_states_new = torch.empty( (0,array_of_states.shape[1]) , device=device, dtype = dtype)\n",
        "#       for move in list_generators:\n",
        "#           array_of_states_tmp = array_of_states[row_indices,move]\n",
        "#           array_of_states_new = torch.concatenate([array_of_states_new, array_of_states_tmp],axis = 0)\n",
        "\n",
        "#       # Take only unique states\n",
        "#       # surprise: THAT IS CRITICAL for beam search performance !!!!\n",
        "#       # if that is not done - beam search  will not find the desired state - quite often\n",
        "#       # The reason - essentianlly beam can degrade, i.e. can be populated by copy of only one state\n",
        "#       # It is surprising that such degradation  happens quite often even for beam_width = 10_000 - but it is indeed so\n",
        "#       array_of_states_new = get_unique_states(array_of_states_new)\n",
        "\n",
        "#       # Check destination state found\n",
        "#       vec_tmp = torch.all(array_of_states_new == state_destination, axis =1) # Compare state_destination and each row array_of_states\n",
        "#       flag_found_destination = torch.any(vec_tmp) # Check for coincidence\n",
        "#       if flag_found_destination:\n",
        "#           print('Found destination state. ', 'i_step:', i_step, ' n_ways:', (vec_tmp).sum())\n",
        "#           break\n",
        "\n",
        "#       # ML-model inference - estimate distance of new states to the destination state\n",
        "#       t0 = time.time()\n",
        "#       if array_of_states_new.shape[0] > beam_width: # If we have not so many states - we take them all - no need for ML-model\n",
        "\n",
        "#           # ML-model inference - estimate distance of new states to the destination state\n",
        "#   #         y_pred = model.predict(array_of_states_new.cpu().numpy() )\n",
        "#           y_pred = make_predictions_on_array(model, array_of_states_new.cpu().numpy() )\n",
        "\n",
        "#           # Take only \"beam_width\" of the best states (i.e. most nearest to destination according to the model estimate)\n",
        "#           idx = np.argsort(y_pred)[:beam_width]\n",
        "#           array_of_states = array_of_states_new[idx,:]\n",
        "\n",
        "#       else:\n",
        "#           array_of_states = array_of_states_new\n",
        "#       predict_time = time.time() - t0\n",
        "#       print(i_step,'i_step', '%.3f'%predict_time, 'predict_time', array_of_states_new.shape, 'array_of_states_new.shape' )\n",
        "\n",
        "#   print();\n",
        "#   print('Search finished.', 'beam_width:', beam_width)\n",
        "#   if flag_found_destination:\n",
        "#       print(i_step, ' steps to destination state. Path found.')\n",
        "#   else:\n",
        "#       print('Path not found.')"
      ],
      "metadata": {
        "id": "BXvzpfpfK9JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def beam_search(state_start, model, beam_width =  10_000, i_step_max = 400, verbose = 0):\n",
        "#     '''\n",
        "#     Finds a path from the state_start to the destination state.\n",
        "\n",
        "#     '''\n",
        "#     # Initialize array of states\n",
        "#     array_of_states = state_start.view(1, state_size  ).clone().to(dtype).to(device)\n",
        "#     state_destination = torch.arange( state_size, device=device, dtype = dtype)\n",
        "#     # i_step_max = 100\n",
        "#     for i_step in range(1,i_step_max+1):\n",
        "\n",
        "#         # Technical preparation: to apply permutations to array we need the trick:\n",
        "#         # naively it is: array[ :, IX_array], but actually we need  to write array[ range(N)[:, np.newaxis], IX_array  ]\n",
        "#         row_indices = np.arange( array_of_states.shape[0] )[:, np.newaxis]\n",
        "\n",
        "#         # Apply all moves to all current states, all new states saved in array_of_states_new\n",
        "#         array_of_states_new = torch.empty( (0,array_of_states.shape[1]) , device=device, dtype = dtype)\n",
        "#         for move in list_generators:\n",
        "#             array_of_states_tmp = array_of_states[row_indices,move]\n",
        "#             array_of_states_new = torch.concatenate([array_of_states_new, array_of_states_tmp],axis = 0)\n",
        "\n",
        "#         # Take only unique states\n",
        "#         # surprise: THAT IS CRITICAL for beam search performance !!!!\n",
        "#         # if that is not done - beam search  will not find the desired state - quite often\n",
        "#         # The reason - essentianlly beam can degrade, i.e. can be populated by copy of only one state\n",
        "#         # It is surprising that such degradation  happens quite often even for beam_width = 10_000 - but it is indeed so\n",
        "#         array_of_states_new = get_unique_states(array_of_states_new)\n",
        "\n",
        "#         # Check destination state found\n",
        "#         vec_tmp = torch.all(array_of_states_new == state_destination, axis =1) # Compare state_destination and each row array_of_states\n",
        "#         flag_found_destination = torch.any(vec_tmp).item() # Check for coincidence\n",
        "#         if flag_found_destination:\n",
        "#             if verbose >= 1:\n",
        "#                 print('Found destination state. ', 'i_step:', i_step, ' n_ways:', (vec_tmp).sum())\n",
        "#             break\n",
        "\n",
        "#         # ML-model inference - estimate distance of new states to the destination state\n",
        "#         t0 = time.time()\n",
        "#         if array_of_states_new.shape[0] > beam_width: # If we have not so many states - we take them all - no need for ML-model\n",
        "\n",
        "#             # ML-model inference - estimate distance of new states to the destination state\n",
        "# #             y_pred = model.predict(array_of_states_new.cpu().numpy() )\n",
        "#             y_pred = make_predictions_on_array(model, array_of_states_new.cpu().numpy() )\n",
        "#             # Take only \"beam_width\" of the best states (i.e. most nearest to destination according to the model estimate)\n",
        "#             idx = np.argsort(y_pred)[:beam_width]\n",
        "#             array_of_states = array_of_states_new[idx,:]\n",
        "\n",
        "#         else:\n",
        "#             array_of_states = array_of_states_new\n",
        "#         predict_time = time.time() - t0\n",
        "#         if verbose >= 10:\n",
        "#             print(i_step,'i_step', '%.3f'%predict_time, 'predict_time', array_of_states_new.shape, 'array_of_states_new.shape' )\n",
        "\n",
        "#     if verbose >= 1:\n",
        "#         print();\n",
        "#         print('Search finished.', 'beam_width:', beam_width)\n",
        "#         if flag_found_destination:\n",
        "#             print(i_step, ' steps to destination state. Path found.')\n",
        "#         else:\n",
        "#             print('Path not found.')\n",
        "\n",
        "#     return flag_found_destination, i_step\n",
        "\n",
        "# def run_beam_search():\n",
        "#     # %%time\n",
        "#   n_scrambles_starting_state = 5\n",
        "#   n_gens = len(list_generators)\n",
        "#   state_destination = torch.arange( state_size, device=device, dtype = dtype)\n",
        "#   # Scramble - generate state which will be start of beam search\n",
        "#   state_start = state_destination\n",
        "#   for k in range(n_scrambles_starting_state):\n",
        "#       IX_move = np.random.randint(0, n_gens, dtype = int) # random moves indixes\n",
        "#       state_start = state_start[ list_generators[IX_move]] # all_moves[IX_moves,:] ]\n",
        "\n",
        "\n",
        "#   flag_found_destination, i_step = beam_search(state_start, model, beam_width =  10, i_step_max = 200,\n",
        "#                                               verbose = 100)\n"
      ],
      "metadata": {
        "id": "w46j-cI0Lecy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def full_stats():\n",
        "#   # n_scrambles_starting_state = 1000\n",
        "#   # print('n_scrambles_starting_state:',n_scrambles_starting_state)\n",
        "\n",
        "#   fig = plt.figure(figsize = (20,8))\n",
        "\n",
        "#   for beam_width in [1, 10]:\n",
        "\n",
        "#       str_inf = 'Catboost + beam_width ' + str(beam_width)\n",
        "\n",
        "#       df_stat = pd.DataFrame()\n",
        "\n",
        "#       for i_trial in range(10):\n",
        "#           if i_trial < 25:\n",
        "#               #print()\n",
        "#               print('trial:', i_trial)\n",
        "#           # destination state - \"solved puzzle state\"\n",
        "#           state_destination = torch.arange( state_size, device=device, dtype = dtype)\n",
        "\n",
        "#           #state_start\n",
        "\n",
        "#           prm_name = 'n_scramble '\n",
        "#           list_prm_value = list(range(1,21))\n",
        "#           for prm_value in list_prm_value:#  5_000,10_000, 30_000]:#, 100_000]:\n",
        "#               n_scrambles_starting_state = prm_value\n",
        "\n",
        "#               # Scramble - generate state which will be start of beam search\n",
        "#               state_start = state_destination\n",
        "#               for k in range(n_scrambles_starting_state):\n",
        "#                   IX_move = np.random.randint(0, n_gens, dtype = int) # random moves indixes\n",
        "#                   state_start = state_start[ list_generators[IX_move]] # all_moves[IX_moves,:] ]\n",
        "\n",
        "#               t0 = time.time()\n",
        "#               flag_found_destination, i_step = beam_search(state_start, model, beam_width =  beam_width, i_step_max = 200,\n",
        "#                                                           verbose = 0)\n",
        "#               if i_trial < 1:\n",
        "#                   print('Found:',flag_found_destination,'steps:', i_step,'beam_width:', beam_width,\n",
        "#                         'time: %.1f secs'%(time.time()-t0))\n",
        "\n",
        "#               df_stat.loc[i_trial,'Path length.  '+prm_name+str(prm_value)] =  i_step\n",
        "#               df_stat.loc[i_trial,'Solution found. '+prm_name +str(prm_value)] =  int(flag_found_destination)\n",
        "#               df_stat.loc[i_trial,'Time. '+prm_name+str(prm_value)] =  np.round(time.time()-t0,1)\n",
        "\n",
        "#       df_stat.to_csv('stat_beam_search_'+prm_name+'_'+str_inf+'.csv')\n",
        "#       display(df_stat)\n",
        "#       display(df_stat.describe().round(3).T)\n",
        "#       display(df_stat.describe().round(3).to_csv('aggregated_stat_beam_search_'+prm_name+'_'+str_inf+'.csv'))\n",
        "\n",
        "\n",
        "#       df_loc = df_stat.describe()\n",
        "#       dat_loc = []\n",
        "#       col_key = 'Solution found'\n",
        "#       for col in df_stat.columns:\n",
        "#           if not( col_key in col) : continue\n",
        "#           dat_loc.append(df_loc.loc['mean', col] )\n",
        "\n",
        "#       plt.plot(list_prm_value, dat_loc, '*-',label = str_inf)\n",
        "#       plt.title(col_key , fontsize = 20  )\n",
        "#       plt.legend(fontsize = 20 )\n",
        "#       plt.savefig(col_key.replace('.', ' ') + '.png')\n",
        "#   plt.grid()\n",
        "#   plt.show()"
      ],
      "metadata": {
        "id": "DxPx--geLrjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JVXVcgUSK9N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z35QeJW897kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MatrixDatasetRegr(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.int64)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = F.one_hot(self.X[idx], num_classes=self.X.shape[1]).float()\n",
        "        return x.unsqueeze(0), self.y[idx]\n",
        "\n",
        "class MyIntSequenceDataset(Dataset):\n",
        "    def __init__(self, X, y, is_regression=True):\n",
        "        # Store integer inputs and targets\n",
        "        dtype_y = torch.float32 if is_regression else torch.long\n",
        "        self.X = torch.tensor(X, dtype=torch.long)  # shape (N, 96)\n",
        "        self.y = torch.tensor(y, dtype=dtype_y)     # shape (N,)\n",
        "\n",
        "        self.is_regression = is_regression\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return (input_sequence, target)\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# train_dataset = MyIntSequenceDataset(X_train, y_train)\n",
        "# test_dataset = MyIntSequenceDataset(X_test, y_test)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "# model = CNNRegr().to(device)\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def calculate_metrics(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in data_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "    return rmse, r2"
      ],
      "metadata": {
        "id": "PVr7ALveBZd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "class Embedded1DCNN_Wider_Seq10(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size=96,  # number of possible integer tokens\n",
        "                 embed_dim=32,   # embedding dimension\n",
        "                 out_dim=60,     # output dimension (e.g. 60 classes, or 1 for regression)\n",
        "                 dropout_p=0.3\n",
        "                ):\n",
        "        \"\"\"\n",
        "        A 'wider' 1D CNN architecture that expects an input sequence length = 10.\n",
        "\n",
        "        Steps:\n",
        "          - Embedding -> dropout\n",
        "          - Conv1 (128 channels), BN, ReLU, dropout -> pool\n",
        "          - Conv2 (256 channels), BN, ReLU, dropout -> pool\n",
        "          - Flatten -> FC (512) -> dropout -> FC (out_dim)\n",
        "\n",
        "        After 2 pools (each with kernel_size=2), the sequence length goes:\n",
        "          10 -> 5 -> 2.\n",
        "\n",
        "        The final feature map is (batch_size, 256, 2),\n",
        "        so fc_input_dim = 256 * 2 = 512.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        # Embedding\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "        self.dropout_embed = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # Convolution layers (wider than usual)\n",
        "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm1d(128)\n",
        "        self.dropout_conv1 = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm1d(256)\n",
        "        self.dropout_conv2 = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # Pooling\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # After 2 pools on length=10 => 10 -> 5 -> 2\n",
        "        # Final shape after conv2+pool: (batch_size, 256, 2)\n",
        "        self.fc_input_dim = 256 * 2\n",
        "\n",
        "        # Fully connected\n",
        "        self.fc1 = nn.Linear(self.fc_input_dim, 512)  # large width\n",
        "        self.dropout_fc1 = nn.Dropout(p=dropout_p)\n",
        "        self.fc2 = nn.Linear(512, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len=10) integer tokens in [0..vocab_size-1].\n",
        "        \"\"\"\n",
        "        # Embedding => (batch_size, 10, embed_dim)\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout_embed(x)\n",
        "\n",
        "        # Permute for Conv1d => (batch_size, embed_dim, 10)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)               # (batch_size, 128, 10)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout_conv1(x)\n",
        "        x = self.pool(x)                # (batch_size, 128, 5)\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)               # (batch_size, 256, 5)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout_conv2(x)\n",
        "        x = self.pool(x)                # (batch_size, 256, 2)\n",
        "\n",
        "        # Flatten => (batch_size, 256*2)\n",
        "        x = x.view(x.size(0), -1)       # => (batch_size, 512)\n",
        "\n",
        "        # Fully connected\n",
        "        x = self.fc1(x)                 # => (batch_size, 512)\n",
        "        x = self.dropout_fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Final output\n",
        "        logits = self.fc2(x)           # => (batch_size, out_dim)\n",
        "        if self.out_dim == 1:\n",
        "          return logits.squeeze(1)\n",
        "        else:\n",
        "          return logits\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qY-_7NfaBa7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedded1DCNN_Flexible(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size=96,   # Number of integer tokens possible\n",
        "                 embed_dim=32,    # Embedding dimension\n",
        "                 out_dim=60,      # Number of output classes (or 1 for regression)\n",
        "                 dropout_p=0.3\n",
        "                ):\n",
        "        \"\"\"\n",
        "        A flexible 1D CNN architecture with:\n",
        "          - Embedding layer\n",
        "          - 2 convolutional blocks (Conv1d -> BN -> ReLU -> Dropout)\n",
        "          - An adaptive average pool to reduce the sequence dimension to 1\n",
        "          - Fully-connected layers for classification\n",
        "\n",
        "        This approach can handle any input length >= 1 (e.g., 10, 20, 40),\n",
        "        because the final 'nn.AdaptiveAvgPool1d(1)' automatically downscales\n",
        "        to a length of 1 regardless of the initial sequence size.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        # Embedding for integer tokens\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "        self.dropout_embed = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # ---- Convolutional Blocks ----\n",
        "        # Block 1: in_channels=embed_dim -> out_channels=128\n",
        "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm1d(128)\n",
        "        self.dropout_conv1 = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # Block 2: in_channels=128 -> out_channels=256\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm1d(256)\n",
        "        self.dropout_conv2 = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # ---- Adaptive Pooling ----\n",
        "        # This layer will convert any (batch, 256, seq_len') to (batch, 256, 1).\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(output_size=1)\n",
        "\n",
        "        # ---- Fully-Connected Head ----\n",
        "        # After pooling => shape (batch_size, 256, 1) => flatten => (batch_size, 256)\n",
        "        self.fc1 = nn.Linear(256, 512)\n",
        "        self.dropout_fc1 = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc2 = nn.Linear(512, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len), each entry is an integer in [0..vocab_size-1].\n",
        "        seq_len can be 10, 20, 40, etc.\n",
        "        \"\"\"\n",
        "        # 1) Embedding => (batch_size, seq_len, embed_dim)\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout_embed(x)\n",
        "\n",
        "        # Permute for Conv1d => (batch_size, embed_dim, seq_len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # 2) Conv Block 1\n",
        "        x = self.conv1(x)                # => (batch_size, 128, seq_len)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout_conv1(x)\n",
        "\n",
        "        # 3) Conv Block 2\n",
        "        x = self.conv2(x)                # => (batch_size, 256, seq_len)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout_conv2(x)\n",
        "\n",
        "        # 4) Global Adaptive Pooling => (batch_size, 256, 1)\n",
        "        x = self.global_pool(x)\n",
        "\n",
        "        # Flatten => (batch_size, 256)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # 5) Fully-connected Layers\n",
        "        x = self.fc1(x)                  # => (batch_size, 512)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout_fc1(x)\n",
        "\n",
        "        # 6) Final classification => (batch_size, out_dim)\n",
        "        logits = self.fc2(x)\n",
        "        if self.out_dim == 1:\n",
        "          return logits.squeeze(1)\n",
        "        else:\n",
        "          return logits"
      ],
      "metadata": {
        "id": "a9bm7w9-U8M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = Embedded1DCNN_Flexible(out_dim=1).to(device)\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.8, verbose=True)\n",
        "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000\n",
        "# print(f'Total trainable parameters: {pytorch_total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCbnO8VaIvW9",
        "outputId": "29d964a1-dec6-4b69-f8ba-7603117d1917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 0.246913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs=2):\n",
        "  # Training loop\n",
        "  train_rmse_list, train_r2_list = [], []\n",
        "  test_rmse_list, test_r2_list = [], []\n",
        "  loss_list = []\n",
        "\n",
        "\n",
        "  best_test_rmse = float('inf')\n",
        "  patience = 5\n",
        "  no_improve = 0\n",
        "  best_model_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  X, _ = random_walks(n_random_walk_length       = n_random_walk_length,\n",
        "                                                      n_random_walks_to_generate = n_random_walks_to_generate)\n",
        "  print(f'{X.shape=}')\n",
        "  del X, _\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      X_train, y_train = random_walks(n_random_walk_length       = n_random_walk_length,\n",
        "                                                      n_random_walks_to_generate = n_random_walks_to_generate)\n",
        "\n",
        "      train_dataset = MyIntSequenceDataset(X_train, y_train)\n",
        "      train_loader = DataLoader(train_dataset, batch_size=256 * 2, shuffle=True)\n",
        "      model.train()\n",
        "      for batch_X, batch_y in train_loader:\n",
        "          batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(batch_X)\n",
        "          loss = criterion(outputs, batch_y)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      # Calculate metrics for train and test data\n",
        "      train_rmse, train_r2 = calculate_metrics(model, train_loader)\n",
        "      test_rmse, test_r2 = calculate_metrics(model, test_loader)\n",
        "      train_rmse_list.append(train_rmse)\n",
        "      train_r2_list.append(train_r2)\n",
        "      test_rmse_list.append(test_rmse)\n",
        "      test_r2_list.append(test_r2)\n",
        "\n",
        "      scheduler.step(test_rmse)\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "      print(f'Train - RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}')\n",
        "      print(f'Test  - RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}')\n",
        "      # Early stopping\n",
        "      if test_rmse < best_test_rmse:\n",
        "          best_test_rmse = test_rmse\n",
        "          no_improve = 0\n",
        "          torch.save(model.state_dict(), f'/content/drive/MyDrive/archive/best_model_{best_test_rmse}.pth')\n",
        "          best_model_weights = copy.deepcopy(model.state_dict())\n",
        "      else:\n",
        "          no_improve += 1\n",
        "          if no_improve == patience:\n",
        "              print(\"Early stopping triggered\")\n",
        "              break\n",
        "  print('Finished Training')\n",
        "  print(f'Best metrics: {min(test_rmse_list)=:.4f} {max(test_r2_list)=:.4f}')\n",
        "  model.load_state_dict(best_model_weights)\n",
        "  return model, train_rmse_list, train_r2_list, test_rmse_list, test_r2_list, loss_list"
      ],
      "metadata": {
        "id": "FJQk8Mt6IuEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dimension = 16\n",
        "# dic={5: 20,\n",
        "#      10: 140,\n",
        "#      16: 180,\n",
        "#     20:\t250,\n",
        "# 21:\t273,\n",
        "# 22:\t297,\n",
        "# 23:\t322,\n",
        "# 24:\t348,\n",
        "# 25:\t375,\n",
        "# 26:\t403,\n",
        "# 27:\t432,\n",
        "# 28:\t462,\n",
        "# 29:\t493,\n",
        "# 30:\t525,\n",
        "# 31:\t558,\n",
        "# 32:\t592,\n",
        "# 35:\t700,\n",
        "# 37:\t777,\n",
        "# 40:\t900,\n",
        "# 45:\t1125,\n",
        "# 50:\t1375,\n",
        "# 60:\t1950,\n",
        "# 70:\t2625,\n",
        "# 80:\t3400,\n",
        "# 90:\t4275,\n",
        "# 100: 5250,\n",
        "# 150:\t11625,\n",
        "# 200:\t20500,\n",
        "# 300:\t45750}\n",
        "# n_random_walk_length = dic[dimension]\n",
        "# rate_of_diffusion_distance =10\n",
        "# n_random_walk_length = n_random_walk_length*rate_of_diffusion_distance\n",
        "# n_random_walks_to_generate_validation_fixed = 100\n",
        "# # n_random_walk_length = 40\n",
        "# n_random_walks_to_generate = 1000\n",
        "\n",
        "# def make_S_N(N):\n",
        "#     return [\n",
        "#         [1,0,]+[q+2 for q in range(N-2)]       ,\n",
        "#                [q+1 for q in range(N-1)] + [0,],\n",
        "#         [N-1,]+[q+0 for q in range(N-1)]       ,\n",
        "#     ]\n",
        "# list_generators = make_S_N(dimension)\n",
        "# X_test, y_test = random_walks(n_random_walk_length       = n_random_walk_length,\n",
        "#                                                      n_random_walks_to_generate = n_random_walks_to_generate)\n",
        "# X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Yk9r54oMGr3",
        "outputId": "20323f9c-6d3b-44d9-aa8d-5a68c4ae7f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1800000, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_rmse_list, train_r2_list, test_rmse_list, test_r2_list, loss_list = train(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0YZD_ywMnTG",
        "outputId": "e666f796-a7bf-4a4d-a3b6-5876e75d3d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape=torch.Size([1800000, 16])\n",
            "Epoch [1/100], Loss: 96527.3828\n",
            "Train - RMSE: 332.1559, R²: 0.5914\n",
            "Test  - RMSE: 288.1650, R²: -2496.4148\n",
            "Epoch [2/100], Loss: 100970.5859\n",
            "Train - RMSE: 341.1747, R²: 0.5689\n",
            "Test  - RMSE: 349.6543, R²: -3675.9373\n",
            "Epoch [3/100], Loss: 132497.2500\n",
            "Train - RMSE: 331.2351, R²: 0.5936\n",
            "Test  - RMSE: 331.7136, R²: -3308.2900\n",
            "Epoch [4/100], Loss: 115068.4375\n",
            "Train - RMSE: 333.8314, R²: 0.5872\n",
            "Test  - RMSE: 350.4673, R²: -3693.0549\n",
            "Epoch [5/100], Loss: 119225.0859\n",
            "Train - RMSE: 327.6299, R²: 0.6024\n",
            "Test  - RMSE: 326.8216, R²: -3211.4011\n",
            "Epoch [6/100], Loss: 124021.6562\n",
            "Train - RMSE: 333.2488, R²: 0.5887\n",
            "Test  - RMSE: 307.0796, R²: -2835.0266\n",
            "Epoch [7/100], Loss: 142338.9688\n",
            "Train - RMSE: 334.8371, R²: 0.5848\n",
            "Test  - RMSE: 272.6524, R²: -2234.7688\n",
            "Epoch [8/100], Loss: 127898.1250\n",
            "Train - RMSE: 344.3389, R²: 0.5609\n",
            "Test  - RMSE: 276.8425, R²: -2304.0146\n",
            "Epoch [9/100], Loss: 99430.2500\n",
            "Train - RMSE: 344.9378, R²: 0.5593\n",
            "Test  - RMSE: 298.8608, R²: -2685.2488\n",
            "Epoch [10/100], Loss: 116519.9844\n",
            "Train - RMSE: 332.9954, R²: 0.5893\n",
            "Test  - RMSE: 281.6357, R²: -2384.5239\n",
            "Epoch [11/100], Loss: 114149.3047\n",
            "Train - RMSE: 331.8124, R²: 0.5922\n",
            "Test  - RMSE: 328.4649, R²: -3243.7874\n",
            "Epoch [12/100], Loss: 116873.7422\n",
            "Train - RMSE: 330.5074, R²: 0.5954\n",
            "Test  - RMSE: 325.1867, R²: -3179.3435\n",
            "Epoch [13/100], Loss: 95412.8438\n",
            "Train - RMSE: 331.4683, R²: 0.5931\n",
            "Test  - RMSE: 289.6300, R²: -2521.8728\n",
            "Epoch [14/100], Loss: 124665.2969\n",
            "Train - RMSE: 341.4846, R²: 0.5681\n",
            "Test  - RMSE: 294.9019, R²: -2614.5535\n",
            "Epoch [15/100], Loss: 120175.2344\n",
            "Train - RMSE: 333.0178, R²: 0.5893\n",
            "Test  - RMSE: 299.4254, R²: -2695.4080\n",
            "Epoch [16/100], Loss: 91636.0234\n",
            "Train - RMSE: 332.8201, R²: 0.5897\n",
            "Test  - RMSE: 321.5387, R²: -3108.3875\n",
            "Epoch [17/100], Loss: 142084.0469\n",
            "Train - RMSE: 335.4944, R²: 0.5831\n",
            "Test  - RMSE: 302.8571, R²: -2757.5693\n",
            "Early stopping triggered\n",
            "Finished Training\n",
            "Best metrics: min(test_rmse_list)=272.6524 max(test_r2_list)=-2234.7688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g21PJusMPA9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_beam_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07CrNQ_yDqeo",
        "outputId": "f0f92c09-4223-4491-df64-d50da31ec8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 i_step 0.000 predict_time torch.Size([3, 40]) array_of_states_new.shape\n",
            "2 i_step 0.000 predict_time torch.Size([7, 40]) array_of_states_new.shape\n",
            "Found destination state.  i_step: 3  n_ways: tensor(1, device='cuda:0')\n",
            "\n",
            "Search finished. beam_width: 10\n",
            "3  steps to destination state. Path found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 32  steps to destination state. Path found.\n",
        "# 5  steps to destination state. Path found."
      ],
      "metadata": {
        "id": "eSTxwj0nDxIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YD3HtdW7C5go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scramble_given_state_custom(list_generators, n_scrambles=1, state_to_scramble='01234...'):\n",
        "\n",
        "    # Если начальное состояние задано по умолчанию, создаём массив из последовательных чисел\n",
        "    if state_to_scramble == '01234...':\n",
        "        state_size = len(list_generators[0])\n",
        "        state_to_scramble = torch.arange(state_size)  # Используем тензор вместо массива\n",
        "\n",
        "    # Приведение к тензору для упрощения операций\n",
        "    if isinstance(state_to_scramble, (list, range)):\n",
        "        state_to_scramble = torch.tensor(state_to_scramble)\n",
        "\n",
        "    state_current = np.asarray(state_to_scramble.cpu())\n",
        "    n = len(state_current)\n",
        "\n",
        "    # Заранее выпишем саму перестановку p:\n",
        "    # p[i] = индекс в state_current, из которого возьмётся элемент на позицию i после перестановки\n",
        "    # Сначала p = [0, 1, 2, ..., n-1], а потом сделаем нужные перестановки:\n",
        "    p = np.arange(n)\n",
        "    # Обмен (1,2)\n",
        "    p[0], p[1] = p[1], p[0]\n",
        "    i = 2\n",
        "    while i < n-i+1:\n",
        "        # print(i, n-i+1)\n",
        "        p[i], p[n-i+1] = p[n-i+1], p[i]\n",
        "        i += 1\n",
        "\n",
        "    # Теперь применим эту перестановку p n_scrambles раз\n",
        "    for _ in range(n_scrambles):\n",
        "        state_current = state_current[p]\n",
        "\n",
        "    return torch.tensor(state_current, device=device)"
      ],
      "metadata": {
        "id": "78vS8dI11GfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# state_size = len(list_generators[0])\n",
        "# hash_vec = torch.randint(-(2**60), 2**60, (state_size,), dtype = torch.int64, device = device )\n",
        "# # hash_vec = torch.randint(0, 10**9, (state_size,), dtype = torch.int64, device = device )\n",
        "\n",
        "# def get_unique_states(states: torch.Tensor) -> torch.Tensor:\n",
        "#     '''\n",
        "#     Return matrix with unique rows for input matrix \"states\"\n",
        "#     I.e. duplicate rows are dropped.\n",
        "#     For fast implementation: we use hashing via scalar/dot product.\n",
        "#     Note: output order of rows is different from the original.\n",
        "#     '''\n",
        "#     # Note: that implementation is 30 times faster than torch.unique(states, dim = 0) - because we use hashes  (see K.Khoruzhii: https://t.me/sberlogasci/10989/15920)\n",
        "#     # Note: torch.unique does not support returning of indices of unique element so we cannot use it\n",
        "#     # That is in contrast to numpy.unique which supports - set: return_index = True\n",
        "\n",
        "\n",
        "#     # Hashing rows of states matrix:\n",
        "#     hashed = torch.sum(hash_vec * states, dim=1) # Compute hashes.\n",
        "#         # It is same as matrix product torch.matmul(hash_vec , states )\n",
        "#         # but pay attention: such code work with GPU for integers\n",
        "#         # While torch.matmul - does not work for GPU for integer data types,\n",
        "#         # since old GPU hardware (before 2020: P100, T4) does not support integer matrix multiplication\n",
        "\n",
        "#     # Sort\n",
        "#     hashed_sorted, idx = torch.sort(hashed)\n",
        "#     # Mask selects elements which are different from the consequite - that is unique elements (since vector is sorted on the previous step)\n",
        "#     mask = torch.concat((torch.tensor([True], device = device), hashed_sorted[1:] - hashed_sorted[:-1] > 0))\n",
        "#     return states[idx][mask]\n",
        "def beam_search1(state_start, model, beam_width =  10_000, i_step_max = 400, verbose = 0, state_size=state_size):\n",
        "    '''\n",
        "    Finds a path from the state_start to the destination state.\n",
        "\n",
        "    '''\n",
        "    # Initialize array of states\n",
        "    print(state_size)\n",
        "    array_of_states = state_start.view(1, state_size  ).clone().to(dtype).to(device)\n",
        "    state_destination = torch.arange( state_size, device=device, dtype = dtype)\n",
        "    # i_step_max = 100\n",
        "    for i_step in range(1,i_step_max+1):\n",
        "\n",
        "        # Technical preparation: to apply permutations to array we need the trick:\n",
        "        # naively it is: array[ :, IX_array], but actually we need  to write array[ range(N)[:, np.newaxis], IX_array  ]\n",
        "        row_indices = np.arange( array_of_states.shape[0] )[:, np.newaxis]\n",
        "\n",
        "        # Apply all moves to all current states, all new states saved in array_of_states_new\n",
        "        array_of_states_new = torch.empty( (0,array_of_states.shape[1]) , device=device, dtype = dtype)\n",
        "        for move in list_generators:\n",
        "            array_of_states_tmp = array_of_states[row_indices,move]\n",
        "            array_of_states_new = torch.concatenate([array_of_states_new, array_of_states_tmp],axis = 0)\n",
        "\n",
        "        # Take only unique states\n",
        "        # surprise: THAT IS CRITICAL for beam search performance !!!!\n",
        "        # if that is not done - beam search  will not find the desired state - quite often\n",
        "        # The reason - essentianlly beam can degrade, i.e. can be populated by copy of only one state\n",
        "        # It is surprising that such degradation  happens quite often even for beam_width = 10_000 - but it is indeed so\n",
        "        array_of_states_new = get_unique_states(array_of_states_new)\n",
        "\n",
        "        # Check destination state found\n",
        "        vec_tmp = torch.all(array_of_states_new == state_destination, axis =1) # Compare state_destination and each row array_of_states\n",
        "        # print(torch.sum(array_of_states_new == state_destination, axis =1).max())\n",
        "        flag_found_destination = torch.any(vec_tmp).item() # Check for coincidence\n",
        "        if flag_found_destination:\n",
        "            if verbose >= 1:\n",
        "                print('Found destination state. ', 'i_step:', i_step, ' n_ways:', (vec_tmp).sum())\n",
        "            break\n",
        "\n",
        "        # ML-model inference - estimate distance of new states to the destination state\n",
        "        t0 = time.time()\n",
        "        if array_of_states_new.shape[0] > beam_width: # If we have not so many states - we take them all - no need for ML-model\n",
        "\n",
        "            # ML-model inference - estimate distance of new states to the destination state\n",
        "#             y_pred = model.predict(array_of_states_new.cpu().numpy() )\n",
        "            y_pred = make_predictions_on_array(model, array_of_states_new.cpu().numpy() )\n",
        "            # Take only \"beam_width\" of the best states (i.e. most nearest to destination according to the model estimate)\n",
        "            idx = np.argsort(y_pred)[:beam_width]\n",
        "            array_of_states = array_of_states_new[idx,:]\n",
        "\n",
        "        else:\n",
        "            array_of_states = array_of_states_new\n",
        "        predict_time = time.time() - t0\n",
        "        if verbose >= 10:\n",
        "          if i_step % 100 == 0:\n",
        "            print(torch.sum(array_of_states_new == state_destination, axis =1).max())\n",
        "            print(i_step,'i_step', '%.3f'%predict_time, 'predict_time', array_of_states_new.shape, 'array_of_states_new.shape' )\n",
        "\n",
        "    if verbose >= 1:\n",
        "        print();\n",
        "        print('Search finished.', 'beam_width:', beam_width)\n",
        "        if flag_found_destination:\n",
        "            print(i_step, ' steps to destination state. Path found.')\n",
        "        else:\n",
        "            print('Path not found.')\n",
        "\n",
        "    return flag_found_destination, i_step\n",
        "\n",
        "def run_beam_search1(beam_width=256*32*4,i_step_max=1000):\n",
        "  state_size = len(list_generators[0])\n",
        "  hash_vec = torch.randint(-(2**60), 2**60, (state_size,), dtype = torch.int64, device = device )\n",
        "  print(hash_vec.shape)\n",
        "  n_scrambles_starting_state = 5\n",
        "  n_gens = len(list_generators)\n",
        "  state_destination = torch.arange( state_size, device=device, dtype = dtype)\n",
        "  # Scramble - generate state which will be start of beam search\n",
        "  state_start = state_destination\n",
        "  # for k in range(n_scrambles_starting_state):\n",
        "  #     IX_move = np.random.randint(0, n_gens, dtype = int) # random moves indixes\n",
        "  #     state_start = state_start[ list_generators[IX_move]] # all_moves[IX_moves,:] ]\n",
        "  state_start = scramble_given_state_custom(list_generators, 1, state_start)\n",
        "  print(state_start.shape)\n",
        "\n",
        "  flag_found_destination, i_step = beam_search1(state_start, model, beam_width = beam_width,\n",
        "                                                i_step_max = 10000,\n",
        "                                              verbose = 100, state_size=state_size)\n"
      ],
      "metadata": {
        "id": "iqjrJpKDxAnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run_beam_search1()"
      ],
      "metadata": {
        "id": "PMRGehpH1e21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5OGvdwJPiwV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_beam_search1(beam_width=131072)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PxuT3stgIe8",
        "outputId": "b948c39b-1074-4fe7-a4a4-38dabc597bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16])\n",
            "torch.Size([16])\n",
            "16\n",
            "tensor(11, device='cuda:0')\n",
            "100 i_step 4.422 predict_time torch.Size([277215, 16]) array_of_states_new.shape\n",
            "Found destination state.  i_step: 128  n_ways: tensor(1, device='cuda:0')\n",
            "\n",
            "Search finished. beam_width: 131072\n",
            "128  steps to destination state. Path found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_beam_search1(beam_width=131072*4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJEUAZ7XiEQs",
        "outputId": "04c20f5a-67cc-4c10-8242-02f7ad79cf11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16])\n",
            "torch.Size([16])\n",
            "16\n",
            "tensor(13, device='cuda:0')\n",
            "100 i_step 17.391 predict_time torch.Size([1083078, 16]) array_of_states_new.shape\n",
            "Found destination state.  i_step: 122  n_ways: tensor(1, device='cuda:0')\n",
            "\n",
            "Search finished. beam_width: 524288\n",
            "122  steps to destination state. Path found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length, beam_width, steps\n",
        "10, 1000, 45\n",
        "16, 32768, 132\n",
        "16, 131072, 128\n",
        "16, 524288, 122"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kshw1MSB1hLp",
        "outputId": "656b5157-b10a-4a59-be98-5002a1b99bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = 40\n",
        "dic={5: 20,\n",
        "     10: 140,\n",
        "     16: 180,\n",
        "    20:\t250,\n",
        "21:\t273,\n",
        "22:\t297,\n",
        "23:\t322,\n",
        "24:\t348,\n",
        "25:\t375,\n",
        "26:\t403,\n",
        "27:\t432,\n",
        "28:\t462,\n",
        "29:\t493,\n",
        "30:\t525,\n",
        "31:\t558,\n",
        "32:\t592,\n",
        "35:\t700,\n",
        "37:\t777,\n",
        "40:\t900,\n",
        "45:\t1125,\n",
        "50:\t1375,\n",
        "60:\t1950,\n",
        "70:\t2625,\n",
        "80:\t3400,\n",
        "90:\t4275,\n",
        "100: 5250,\n",
        "150:\t11625,\n",
        "200:\t20500,\n",
        "300:\t45750}\n",
        "n_random_walk_length = dic[dimension]\n",
        "rate_of_diffusion_distance =10\n",
        "n_random_walk_length = n_random_walk_length*rate_of_diffusion_distance\n",
        "n_random_walks_to_generate_validation_fixed = 100\n",
        "# n_random_walk_length = 40\n",
        "n_random_walks_to_generate = 1000\n",
        "\n",
        "def make_S_N(N):\n",
        "    return [\n",
        "        [1,0,]+[q+2 for q in range(N-2)]       ,\n",
        "               [q+1 for q in range(N-1)] + [0,],\n",
        "        [N-1,]+[q+0 for q in range(N-1)]       ,\n",
        "    ]\n",
        "list_generators = make_S_N(dimension)\n",
        "X_test, y_test = random_walks(n_random_walk_length       = n_random_walk_length,\n",
        "                                                     n_random_walks_to_generate = n_random_walks_to_generate)\n",
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HivaprdxBf7",
        "outputId": "65ee3c1f-7735-49fa-c545-c6c7afbbd0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9000000, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock1D(nn.Module):\n",
        "    \"\"\"\n",
        "    A single residual block for 1D signals:\n",
        "      - Conv1D -> BN -> ReLU -> Dropout -> Conv1D -> BN\n",
        "      - skip connection\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2  # same padding\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
        "        self.bn1   = nn.BatchNorm1d(out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding)\n",
        "        self.bn2   = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        # If in_channels != out_channels, we need a 1x1 conv to match dimensions for the skip\n",
        "        self.shortcut = None\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Match channels if needed\n",
        "        if self.shortcut is not None:\n",
        "            residual = self.shortcut(residual)\n",
        "\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet1D(nn.Module):\n",
        "    \"\"\"\n",
        "    A ResNet-style 1D CNN for sequences of variable length:\n",
        "      1) Embedding (optional, if inputs are integer tokens)\n",
        "      2) Initial Conv1D\n",
        "      3) Multiple ResidualBlock1D layers\n",
        "      4) Global AdaptiveAvgPool1d(1)\n",
        "      5) Fully-connected output\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size=100,\n",
        "                 embed_dim=32,\n",
        "                 hidden_dims=[64, 128, 128],\n",
        "                 out_dim=1,\n",
        "                 dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        # Embedding: for integer input sequences\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Initial projection from embed_dim -> hidden_dims[0]\n",
        "        self.input_conv = nn.Conv1d(embed_dim, hidden_dims[0], kernel_size=3, padding=1)\n",
        "        self.bn_input   = nn.BatchNorm1d(hidden_dims[0])\n",
        "\n",
        "        # Create a stack of residual blocks\n",
        "        blocks = []\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            in_ch  = hidden_dims[i]\n",
        "            out_ch = hidden_dims[i+1]\n",
        "            block  = ResidualBlock1D(in_ch, out_ch, kernel_size=3, dropout=dropout)\n",
        "            blocks.append(block)\n",
        "        self.res_blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        # Global pooling to get a single vector\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Final linear layer\n",
        "        self.fc = nn.Linear(hidden_dims[-1], out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len) with integer tokens in [0..vocab_size-1]\n",
        "        \"\"\"\n",
        "        # 1) Embed => (batch, seq_len, embed_dim)\n",
        "        x = self.embedding(x)\n",
        "        # 2) Permute => (batch, embed_dim, seq_len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # 3) Input conv -> BN -> ReLU\n",
        "        x = self.input_conv(x)\n",
        "        x = self.bn_input(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # 4) Pass through residual blocks\n",
        "        x = self.res_blocks(x)  # shape still (batch, hidden_dims[-1], seq_len)\n",
        "\n",
        "        # 5) Global pooling => (batch, hidden_dims[-1], 1)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.squeeze(-1)       # => (batch, hidden_dims[-1])\n",
        "\n",
        "        # 6) Final FC => (batch, out_dim)\n",
        "        out = self.fc(x)\n",
        "        if self.out_dim == 1:\n",
        "            out = out.squeeze(-1)  # scalar for each batch element\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZGM8q1gI9ub2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = ResNet1D(vocab_size=dimension, embed_dim=64, hidden_dims=[64, 128, 128, 256, 256]).to(device)\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.8, verbose=True)\n",
        "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000\n",
        "# print(f'Total trainable parameters: {pytorch_total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_tJanznwnLm",
        "outputId": "1844624f-eb9c-4ede-940c-3e48ea5fad6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 0.921409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = ResNet1D(vocab_size=dimension, embed_dim=128, hidden_dims=[64, 128, 128, 256, 256, 512, 512]).to(device)\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.8, verbose=True)\n",
        "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000\n",
        "# print(f'Total trainable parameters: {pytorch_total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QokcfF0t-KI9",
        "outputId": "bb828333-d588-4dbb-cd64-4700741e75ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 3.826753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MyIntSequenceDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "LtiDACKaR0ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls /content/drive/MyDrive/archive/"
      ],
      "metadata": {
        "id": "339YCj67GWXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_test_rmse = '1028.5249'\n",
        "model_path = f'/content/drive/MyDrive/archive/best_model_1028.52485385624.pth'\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZcOyyeOGb5c",
        "outputId": "c8dae326-313f-439a-a46e-bf461e57b844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_rmse_list, train_r2_list, test_rmse_list, test_r2_list, loss_list = train(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdqka6SOs5Mw",
        "outputId": "cbe70178-3fde-4878-8faf-a6b6208ee5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape=torch.Size([9000000, 40])\n",
            "Epoch [1/100], Loss: 53044.4336\n",
            "Train - RMSE: 181.6841, R²: 0.9951\n",
            "Test  - RMSE: 1088.3258, R²: 0.8245\n",
            "Epoch [2/100], Loss: 42284.2578\n",
            "Train - RMSE: 157.3925, R²: 0.9963\n",
            "Test  - RMSE: 1069.0997, R²: 0.8307\n",
            "Epoch [3/100], Loss: 39155.0234\n",
            "Train - RMSE: 149.4759, R²: 0.9967\n",
            "Test  - RMSE: 1056.1937, R²: 0.8347\n",
            "Epoch [4/100], Loss: 34024.4414\n",
            "Train - RMSE: 188.0088, R²: 0.9948\n",
            "Test  - RMSE: 1048.3807, R²: 0.8372\n",
            "Epoch [5/100], Loss: 26315.2012\n",
            "Train - RMSE: 167.2942, R²: 0.9959\n",
            "Test  - RMSE: 1054.4815, R²: 0.8353\n",
            "Epoch [6/100], Loss: 72281.7500\n",
            "Train - RMSE: 182.3396, R²: 0.9951\n",
            "Test  - RMSE: 1046.6954, R²: 0.8377\n",
            "Epoch [7/100], Loss: 65036.2891\n",
            "Train - RMSE: 167.0597, R²: 0.9959\n",
            "Test  - RMSE: 1036.3456, R²: 0.8409\n",
            "Epoch [8/100], Loss: 34897.2969\n",
            "Train - RMSE: 170.4840, R²: 0.9957\n",
            "Test  - RMSE: 1049.6358, R²: 0.8368\n",
            "Epoch [9/100], Loss: 32782.3594\n",
            "Train - RMSE: 166.0198, R²: 0.9959\n",
            "Test  - RMSE: 1040.1753, R²: 0.8397\n",
            "Epoch [10/100], Loss: 32705.2930\n",
            "Train - RMSE: 146.2262, R²: 0.9968\n",
            "Test  - RMSE: 1037.4699, R²: 0.8405\n",
            "Epoch [11/100], Loss: 31609.2246\n",
            "Train - RMSE: 145.7699, R²: 0.9969\n",
            "Test  - RMSE: 1028.5249, R²: 0.8433\n",
            "Epoch [12/100], Loss: 25227.7422\n",
            "Train - RMSE: 142.8272, R²: 0.9970\n",
            "Test  - RMSE: 1034.1537, R²: 0.8416\n",
            "Epoch [13/100], Loss: 22430.4160\n",
            "Train - RMSE: 145.5541, R²: 0.9969\n",
            "Test  - RMSE: 1035.6248, R²: 0.8411\n",
            "Epoch [14/100], Loss: 29992.4922\n",
            "Train - RMSE: 142.5118, R²: 0.9970\n",
            "Test  - RMSE: 1034.5629, R²: 0.8414\n",
            "Epoch [15/100], Loss: 38339.8789\n",
            "Train - RMSE: 158.8073, R²: 0.9963\n",
            "Test  - RMSE: 1032.7059, R²: 0.8420\n",
            "Epoch [16/100], Loss: 26782.0059\n",
            "Train - RMSE: 150.9811, R²: 0.9966\n",
            "Test  - RMSE: 1039.0426, R²: 0.8401\n",
            "Early stopping triggered\n",
            "Finished Training\n",
            "Best metrics: min(test_rmse_list)=1028.5249 max(test_r2_list)=0.8433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLE7OCFrEjuZ",
        "outputId": "4d85dfc7-5995-4f86-c791-6beb21042039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# state_size = len(list_generators[0])\n",
        "# hash_vec = torch.randint(-(2**60), 2**60, (state_size,), dtype = torch.int64, device = device )\n",
        "# # hash_vec = torch.randint(0, 10**9, (state_size,), dtype = torch.int64, device = device )\n",
        "\n",
        "# def get_unique_states(states: torch.Tensor) -> torch.Tensor:\n",
        "#     '''\n",
        "#     Return matrix with unique rows for input matrix \"states\"\n",
        "#     I.e. duplicate rows are dropped.\n",
        "#     For fast implementation: we use hashing via scalar/dot product.\n",
        "#     Note: output order of rows is different from the original.\n",
        "#     '''\n",
        "#     # Note: that implementation is 30 times faster than torch.unique(states, dim = 0) - because we use hashes  (see K.Khoruzhii: https://t.me/sberlogasci/10989/15920)\n",
        "#     # Note: torch.unique does not support returning of indices of unique element so we cannot use it\n",
        "#     # That is in contrast to numpy.unique which supports - set: return_index = True\n",
        "\n",
        "\n",
        "#     # Hashing rows of states matrix:\n",
        "#     hashed = torch.sum(hash_vec * states, dim=1) # Compute hashes.\n",
        "#         # It is same as matrix product torch.matmul(hash_vec , states )\n",
        "#         # but pay attention: such code work with GPU for integers\n",
        "#         # While torch.matmul - does not work for GPU for integer data types,\n",
        "#         # since old GPU hardware (before 2020: P100, T4) does not support integer matrix multiplication\n",
        "\n",
        "#     # Sort\n",
        "#     hashed_sorted, idx = torch.sort(hashed)\n",
        "#     # Mask selects elements which are different from the consequite - that is unique elements (since vector is sorted on the previous step)\n",
        "#     mask = torch.concat((torch.tensor([True], device = device), hashed_sorted[1:] - hashed_sorted[:-1] > 0))\n",
        "#     return states[idx][mask]\n",
        "# def beam_search1(state_start, model, beam_width =  10_000, i_step_max = 400, verbose = 0, state_size=state_size):\n",
        "#     '''\n",
        "#     Finds a path from the state_start to the destination state.\n",
        "\n",
        "#     '''\n",
        "#     # Initialize array of states\n",
        "#     print(state_size)\n",
        "#     array_of_states = state_start.view(1, state_size  ).clone().to(dtype).to(device)\n",
        "#     state_destination = torch.arange( state_size, device=device, dtype = dtype)\n",
        "#     # i_step_max = 100\n",
        "#     for i_step in range(1,i_step_max+1):\n",
        "\n",
        "#         # Technical preparation: to apply permutations to array we need the trick:\n",
        "#         # naively it is: array[ :, IX_array], but actually we need  to write array[ range(N)[:, np.newaxis], IX_array  ]\n",
        "#         row_indices = np.arange( array_of_states.shape[0] )[:, np.newaxis]\n",
        "\n",
        "#         # Apply all moves to all current states, all new states saved in array_of_states_new\n",
        "#         array_of_states_new = torch.empty( (0,array_of_states.shape[1]) , device=device, dtype = dtype)\n",
        "#         for move in list_generators:\n",
        "#             array_of_states_tmp = array_of_states[row_indices,move]\n",
        "#             array_of_states_new = torch.concatenate([array_of_states_new, array_of_states_tmp],axis = 0)\n",
        "\n",
        "#         # Take only unique states\n",
        "#         # surprise: THAT IS CRITICAL for beam search performance !!!!\n",
        "#         # if that is not done - beam search  will not find the desired state - quite often\n",
        "#         # The reason - essentianlly beam can degrade, i.e. can be populated by copy of only one state\n",
        "#         # It is surprising that such degradation  happens quite often even for beam_width = 10_000 - but it is indeed so\n",
        "#         array_of_states_new = get_unique_states(array_of_states_new)\n",
        "\n",
        "#         # Check destination state found\n",
        "#         vec_tmp = torch.all(array_of_states_new == state_destination, axis =1) # Compare state_destination and each row array_of_states\n",
        "#         # print(torch.sum(array_of_states_new == state_destination, axis =1).max())\n",
        "#         flag_found_destination = torch.any(vec_tmp).item() # Check for coincidence\n",
        "#         if flag_found_destination:\n",
        "#             if verbose >= 1:\n",
        "#                 print('Found destination state. ', 'i_step:', i_step, ' n_ways:', (vec_tmp).sum())\n",
        "#             break\n",
        "\n",
        "#         # ML-model inference - estimate distance of new states to the destination state\n",
        "#         t0 = time.time()\n",
        "#         if array_of_states_new.shape[0] > beam_width: # If we have not so many states - we take them all - no need for ML-model\n",
        "\n",
        "#             # ML-model inference - estimate distance of new states to the destination state\n",
        "# #             y_pred = model.predict(array_of_states_new.cpu().numpy() )\n",
        "#             y_pred = make_predictions_on_array(model, array_of_states_new.cpu().numpy() )\n",
        "#             # Take only \"beam_width\" of the best states (i.e. most nearest to destination according to the model estimate)\n",
        "#             idx = np.argsort(y_pred)[:beam_width]\n",
        "#             array_of_states = array_of_states_new[idx,:]\n",
        "\n",
        "#         else:\n",
        "#             array_of_states = array_of_states_new\n",
        "#         predict_time = time.time() - t0\n",
        "#         if verbose >= 10:\n",
        "#           if i_step % 100 == 0:\n",
        "#             print(torch.sum(array_of_states_new == state_destination, axis =1).max())\n",
        "#             print(i_step,'i_step', '%.3f'%predict_time, 'predict_time', array_of_states_new.shape, 'array_of_states_new.shape' )\n",
        "\n",
        "#     if verbose >= 1:\n",
        "#         print();\n",
        "#         print('Search finished.', 'beam_width:', beam_width)\n",
        "#         if flag_found_destination:\n",
        "#             print(i_step, ' steps to destination state. Path found.')\n",
        "#         else:\n",
        "#             print('Path not found.')\n",
        "\n",
        "#     return flag_found_destination, i_step\n",
        "\n",
        "# def run_beam_search1(beam_width=256*32*4):\n",
        "#     # %%time\n",
        "#   state_size = len(list_generators[0])\n",
        "#   hash_vec = torch.randint(-(2**60), 2**60, (state_size,), dtype = torch.int64, device = device )\n",
        "#   print(hash_vec.shape)\n",
        "#   n_scrambles_starting_state = 5\n",
        "#   n_gens = len(list_generators)\n",
        "#   state_destination = torch.arange( state_size, device=device, dtype = dtype)\n",
        "#   # Scramble - generate state which will be start of beam search\n",
        "#   state_start = state_destination\n",
        "#   # for k in range(n_scrambles_starting_state):\n",
        "#   #     IX_move = np.random.randint(0, n_gens, dtype = int) # random moves indixes\n",
        "#   #     state_start = state_start[ list_generators[IX_move]] # all_moves[IX_moves,:] ]\n",
        "#   state_start = scramble_given_state_custom(list_generators, 1, state_start)\n",
        "#   print(state_start.shape)\n",
        "\n",
        "#   flag_found_destination, i_step = beam_search1(state_start, model, beam_width = beam_width,\n",
        "#                                                 i_step_max = 1000,\n",
        "#                                               verbose = 100, state_size=state_size)\n"
      ],
      "metadata": {
        "id": "RPST5y9Ps5QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_beam_search1(beam_width=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_VtDP3qs5Vm",
        "outputId": "4a3882f2-404c-47f7-9a5f-cb14b1fbf2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20])\n",
            "torch.Size([20])\n",
            "20\n",
            "tensor(2, device='cuda:0')\n",
            "100 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "200 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "300 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "400 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "500 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "600 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "700 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "800 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "900 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "tensor(2, device='cuda:0')\n",
            "1000 i_step 0.003 predict_time torch.Size([24, 20]) array_of_states_new.shape\n",
            "\n",
            "Search finished. beam_width: 10\n",
            "Path not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_beam_search1(beam_width=131072)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax0-ojjY5xJD",
        "outputId": "1c46356d-513f-4fdf-ad8b-684143523dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20])\n",
            "torch.Size([20])\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions_on_array(model, array):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    dataset = MyIntSequenceDatasetSmall(array)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for batch_X in loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "\n",
        "    return all_preds"
      ],
      "metadata": {
        "id": "82YGmI0fGyG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "def beam_search_sorting(permutation, beam_width=5):\n",
        "    n = len(permutation)\n",
        "    target = list(range(0, n))\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def average_neighbor_difference(perm):\n",
        "        total_diff = 0\n",
        "\n",
        "        # Считаем разницы между соседними элементами\n",
        "        for i in range(n-1):\n",
        "            total_diff += abs(perm[i] - perm[i+1])\n",
        "\n",
        "        # Добавляем разницу между последним и первым элементом\n",
        "        total_diff += abs(perm[-1] - perm[0])\n",
        "\n",
        "        return total_diff / n\n",
        "\n",
        "    def count_monotonic_sections(perm):\n",
        "        # Создаем расширенную перестановку для учета цикличности\n",
        "        extended_perm = perm + perm[0:1]\n",
        "\n",
        "        sections = 0\n",
        "        increasing = None\n",
        "\n",
        "        for i in range(n):\n",
        "            if increasing is None:  # Первое сравнение\n",
        "                increasing = extended_perm[i] < extended_perm[i + 1]\n",
        "            elif increasing and extended_perm[i] > extended_perm[i + 1]:  # Смена возрастания на убывание\n",
        "                sections += 1\n",
        "                increasing = False\n",
        "            elif not increasing and extended_perm[i] < extended_perm[i + 1]:  # Смена убывания на возрастание\n",
        "                sections += 1\n",
        "                increasing = True\n",
        "\n",
        "        return sections / n\n",
        "\n",
        "    def apply_actions(state):\n",
        "        # Generate new states based on the allowed actions\n",
        "        states = []\n",
        "        # 1. Cyclic shift right\n",
        "        new_state = state[-1:] + state[:-1]\n",
        "        states.append((new_state, \"R\"))\n",
        "\n",
        "        # 2. Cyclic shift left\n",
        "        new_state = state[1:] + state[:1]\n",
        "        states.append((new_state, \"L\"))\n",
        "\n",
        "        # 3. Swap first two elements\n",
        "        if len(state) > 1:\n",
        "            new_state = state[:]\n",
        "            new_state[0], new_state[1] = new_state[1], new_state[0]\n",
        "            states.append((new_state, \"X\"))\n",
        "\n",
        "        return states\n",
        "\n",
        "    def apply_action(state, action):\n",
        "        # 1. Cyclic shift right\n",
        "        if action == 'R':\n",
        "            new_state = state[-1:] + state[:-1]\n",
        "            return new_state\n",
        "        elif action == 'L':\n",
        "            # 2. Cyclic shift left\n",
        "            new_state = state[1:] + state[:1]\n",
        "            return new_state\n",
        "\n",
        "        new_state = state[:]\n",
        "        new_state[0], new_state[1] = new_state[1], new_state[0]\n",
        "        return new_state\n",
        "\n",
        "\n",
        "    # Priority queue for the beam search; stores tuples of (cumulative cost, path, current state)\n",
        "    queue = deque([(0, [], permutation)])\n",
        "    seen = set()\n",
        "\n",
        "    max_actions = 0\n",
        "    q_values = []\n",
        "\n",
        "    while queue:\n",
        "        # Limit the size of the queue as per beam width\n",
        "        queue = deque(sorted(list(queue), key=lambda x: x[0])[:beam_width])\n",
        "        next_queue = deque()\n",
        "\n",
        "        for cost, path, current in queue:\n",
        "            if len(path) > max_actions:\n",
        "                max_actions = len(path)\n",
        "                if max_actions % 100 == 0:\n",
        "                    print('new max len', len(path), len(queue), len(seen), np.min(q_values))\n",
        "                q_values = []\n",
        "            if len(path) > 40000:\n",
        "                #print('long path', queue)\n",
        "                return None\n",
        "            if current == target:\n",
        "                return path  # Return the path to sorted order\n",
        "\n",
        "\n",
        "            for action in ['L','R','X']:\n",
        "                if action == 'L' and len(path) > 0 and path[-1] == 'R':\n",
        "                    continue\n",
        "                if action == 'R' and len(path) > 0 and path[-1] == 'L':\n",
        "                    continue\n",
        "                if action == 'X' and current[0] < current[1]:\n",
        "                    continue\n",
        "\n",
        "                next_state = apply_action(current, action)\n",
        "\n",
        "                if tuple(next_state) not in seen:\n",
        "                    seen.add(tuple(next_state))\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        t_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
        "                        q_value = make_predictions_on_array(model, t_state.cpu().numpy() )\n",
        "                        # q_value = model(t_state).squeeze().item()\n",
        "                        q_values.append(q_value)\n",
        "\n",
        "                    total_cost = cost + 1 + q_value\n",
        "                    next_queue.append((total_cost, path + [action], next_state))\n",
        "\n",
        "        queue = next_queue\n",
        "\n",
        "    return None  # Return None if no solution is found\n",
        "\n",
        "# Пример использования\n",
        "p = np.arange(dimension)\n",
        "n = dimension\n",
        "p[0], p[1] = p[1], p[0]\n",
        "i = 2\n",
        "while i < n-i+1:\n",
        "    p[i], p[n-i+1] = p[n-i+1], p[i]\n",
        "    i += 1\n",
        "permutation = p.tolist()\n",
        "\n",
        "# k = 10  # Ширина \"луча\"\n",
        "# model.eval();\n",
        "# path = beam_search_sorting(permutation, beam_width=k)\n",
        "# if path:\n",
        "#   print(\"Кратчайший путь:\", len(path))\n",
        "#   print(path)\n",
        "# else:\n",
        "#   print('No path found')\n"
      ],
      "metadata": {
        "id": "ykS2sac46mhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k = 100\n",
        "# path = beam_search_sorting(permutation, beam_width=k)\n",
        "# if path:\n",
        "#   print(\"Кратчайший путь:\", len(path))\n",
        "#   print(path)\n",
        "# else:\n",
        "#   print('No path found')"
      ],
      "metadata": {
        "id": "qISArNQM9Qi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 1000\n",
        "path = beam_search_sorting(permutation, beam_width=k)\n",
        "if path:\n",
        "  print(\"Кратчайший путь:\", len(path))\n",
        "  print(path)\n",
        "else:\n",
        "  print('No path found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "2SQMbwowF-81",
        "outputId": "42ce8085-f983-4db8-e56d-980bad4e4404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new max len 100 1000 160346 3098.1602\n",
            "new max len 200 1000 323851 3279.9788\n",
            "new max len 300 1000 482047 3461.4033\n",
            "new max len 400 1000 630470 3728.8447\n",
            "new max len 500 1000 769213 3841.1826\n",
            "new max len 600 1000 910599 4147.19\n",
            "new max len 700 1000 1046679 3923.6255\n",
            "new max len 800 1000 1185759 4047.1436\n",
            "new max len 900 1000 1336775 4105.2744\n",
            "new max len 1000 1000 1481288 3311.1409\n",
            "new max len 1100 1000 1632446 3189.753\n",
            "new max len 1200 1000 1776717 1575.9518\n",
            "new max len 1300 1000 1908387 1542.0941\n",
            "new max len 1400 1000 2035682 1760.7522\n",
            "new max len 1500 1000 2161028 1240.5491\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5e86b84daee7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search_sorting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Кратчайший путь:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-163c520bdad1>\u001b[0m in \u001b[0;36mbeam_search_sorting\u001b[0;34m(permutation, beam_width)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_predictions_on_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;31m# q_value = model(t_state).squeeze().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                         \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-67394bdee2d8>\u001b[0m in \u001b[0;36mmake_predictions_on_array\u001b[0;34m(model, array)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_X\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-7c8205b63b92>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# 4) Pass through residual blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape still (batch, hidden_dims[-1], seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# 5) Global pooling => (batch, hidden_dims[-1], 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-7c8205b63b92>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Match channels if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n\u001b[0;32m--> 370\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kz6_tdf4HOvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXFu9IEkNyBH",
        "outputId": "e0947d94-a8c0-4361-c48c-f23a85308ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20 - fastest 246.\n"
      ],
      "metadata": {
        "id": "prOxA8vmQ7hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD65zhi0RPo4",
        "outputId": "64b54caa-f555-41b4-a50f-d0f88203625f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GatedResidualBlock1D(nn.Module):\n",
        "    \"\"\"\n",
        "    A 1D TCN block with gated activation:\n",
        "      - Conv1D (dilation) -> BN -> [sigmoid gate * tanh candidate] -> Dropout -> Conv1D ...\n",
        "      - Residual skip\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        pad = (kernel_size - 1) * dilation // 2\n",
        "\n",
        "        # First conv for the gated activation\n",
        "        self.conv_filter = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                     padding=pad, dilation=dilation)\n",
        "        self.conv_gate   = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                     padding=pad, dilation=dilation)\n",
        "        self.bn_filter = nn.BatchNorm1d(out_channels)\n",
        "        self.bn_gate   = nn.BatchNorm1d(out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Second conv\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
        "                               padding=pad, dilation=dilation)\n",
        "        self.bn2   = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.shortcut = None\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv1d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        # Gated activation\n",
        "        filter_out = self.conv_filter(x)\n",
        "        filter_out = self.bn_filter(filter_out)\n",
        "        gate_out   = self.conv_gate(x)\n",
        "        gate_out   = self.bn_gate(gate_out)\n",
        "\n",
        "        filter_out = torch.tanh(filter_out)\n",
        "        gate_out   = torch.sigmoid(gate_out)\n",
        "        out = filter_out * gate_out  # gating\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Second conv\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Residual\n",
        "        if self.shortcut is not None:\n",
        "            residual = self.shortcut(residual)\n",
        "\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class GatedTCN1D(nn.Module):\n",
        "    \"\"\"\n",
        "    An improved TCN for 1D sequences using gated residual blocks.\n",
        "      1) Embedding (if needed)\n",
        "      2) Stacked GatedResidualBlock1D with dilations 1, 2, 4, ...\n",
        "      3) Global pool\n",
        "      4) FC for output\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size=50,\n",
        "                 embed_dim=32,\n",
        "                 hidden_dim=128,\n",
        "                 num_levels=4,\n",
        "                 kernel_size=3,\n",
        "                 dropout=0.3,\n",
        "                 out_dim=1):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        # Embedding for integer tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.proj = nn.Conv1d(embed_dim, hidden_dim, kernel_size=1)\n",
        "\n",
        "        # Build stacked blocks with growing dilation\n",
        "        self.blocks = nn.ModuleList()\n",
        "        dilation = 1\n",
        "        for _ in range(num_levels):\n",
        "            block = GatedResidualBlock1D(\n",
        "                in_channels=hidden_dim,\n",
        "                out_channels=hidden_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                dilation=dilation,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "            dilation *= 2\n",
        "\n",
        "        # Global pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Final linear\n",
        "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len) integer tokens\n",
        "        \"\"\"\n",
        "        # 1) Embedding -> (batch, seq_len, embed_dim)\n",
        "        x = self.embedding(x)\n",
        "        # 2) => (batch, embed_dim, seq_len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # 3) Project to hidden_dim\n",
        "        x = self.proj(x)\n",
        "\n",
        "        # 4) Pass through each gated TCN block\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # 5) Global pool => (batch, hidden_dim, 1)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.squeeze(-1)  # => (batch, hidden_dim)\n",
        "\n",
        "        # 6) Output\n",
        "        out = self.fc(x)\n",
        "        if self.out_dim == 1:\n",
        "            out = out.squeeze(-1)\n",
        "        return out\n",
        "\n",
        "# # ----------------------------------\n",
        "# # EXAMPLE USAGE\n",
        "# # ----------------------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     batch_size = 4\n",
        "#     seq_len = 16\n",
        "#     X = torch.randint(0, 50, (batch_size, seq_len))\n",
        "#     y = torch.randn(batch_size)\n",
        "\n",
        "#     model = GatedTCN1D(vocab_size=50, embed_dim=32, hidden_dim=128,\n",
        "#                        num_levels=4, kernel_size=3, dropout=0.3, out_dim=1)\n",
        "#     print(model)\n",
        "#     pred = model(X)\n",
        "#     print(\"Output shape:\", pred.shape)\n",
        "\n",
        "#     criterion = nn.MSELoss()\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "#     loss = criterion(pred, y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "id": "p4BNsDJL-lgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ImXnQQkodSJC",
        "outputId": "7877ee49-305a-4ba6-b19b-6c8130d472cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d2ba684acd0f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = GatedTCN1D(vocab_size=40, embed_dim=32, hidden_dim=128,\n",
        "#                        num_levels=4, kernel_size=3, dropout=0.3, out_dim=1).to(device)\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.8, verbose=True)\n",
        "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000\n",
        "# print(f'Total trainable parameters: {pytorch_total_params}')\n",
        "# model, train_rmse_list, train_r2_list, test_rmse_list, test_r2_list, loss_list = train(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUTsj-cz-p9m",
        "outputId": "309066a1-2e53-4910-f2b4-227251fb0a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 0.600065\n",
            "X.shape=torch.Size([9000000, 40])\n",
            "Epoch [1/100], Loss: 154755.9375\n",
            "Train - RMSE: 292.4204, R²: 0.9873\n",
            "Test  - RMSE: 1084.7953, R²: 0.8257\n",
            "Epoch [2/100], Loss: 97282.0547\n",
            "Train - RMSE: 286.6115, R²: 0.9878\n",
            "Test  - RMSE: 1082.7522, R²: 0.8263\n",
            "Epoch [3/100], Loss: 76442.2031\n",
            "Train - RMSE: 372.5493, R²: 0.9794\n",
            "Test  - RMSE: 1116.3921, R²: 0.8154\n",
            "Epoch [4/100], Loss: 58801.4375\n",
            "Train - RMSE: 351.3758, R²: 0.9817\n",
            "Test  - RMSE: 1116.1797, R²: 0.8154\n",
            "Epoch [5/100], Loss: 82697.8203\n",
            "Train - RMSE: 402.1689, R²: 0.9760\n",
            "Test  - RMSE: 1134.9644, R²: 0.8092\n",
            "Epoch [6/100], Loss: 72408.7031\n",
            "Train - RMSE: 420.1852, R²: 0.9738\n",
            "Test  - RMSE: 1130.4998, R²: 0.8107\n",
            "Epoch [7/100], Loss: 95541.1250\n",
            "Train - RMSE: 390.9566, R²: 0.9774\n",
            "Test  - RMSE: 1102.8646, R²: 0.8198\n",
            "Early stopping triggered\n",
            "Finished Training\n",
            "Best metrics: min(test_rmse_list)=1082.7522 max(test_r2_list)=0.8263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10  # Ширина \"луча\"\n",
        "model.eval();\n",
        "path = beam_search_sorting(permutation, beam_width=k)\n",
        "if path:\n",
        "  print(\"Кратчайший путь:\", len(path))\n",
        "  print(path)\n",
        "else:\n",
        "  print('No path found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJm5ZOcm-8XQ",
        "outputId": "ebad0cbb-7c0d-4131-a435-f260481eb95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new max len 100 10 1723 7660.8794\n",
            "new max len 200 10 3453 7051.2144\n",
            "new max len 300 10 5017 7110.5254\n",
            "new max len 400 10 6606 6834.5244\n",
            "new max len 500 10 8174 5918.989\n",
            "new max len 600 10 9700 6091.792\n",
            "new max len 700 10 11196 5626.2476\n",
            "new max len 800 10 12633 5451.3745\n",
            "new max len 900 10 14050 4583.1235\n",
            "new max len 1000 10 15480 3416.4167\n",
            "new max len 1100 10 16817 2520.675\n",
            "new max len 1200 10 18127 2888.3318\n",
            "new max len 1300 10 19463 1795.1128\n",
            "new max len 1400 10 20759 335.62872\n",
            "new max len 1500 10 22077 137.15617\n",
            "new max len 1600 10 23292 53.02608\n",
            "new max len 1700 10 24520 189.29395\n",
            "new max len 1800 10 25696 251.09688\n",
            "new max len 1900 10 26947 107.45617\n",
            "new max len 2000 10 28205 339.42084\n",
            "new max len 2100 10 29426 827.9243\n",
            "new max len 2200 10 30638 1172.1304\n",
            "new max len 2300 10 31903 1555.2037\n",
            "new max len 2400 10 33232 1858.5203\n",
            "new max len 2500 10 34590 2150.0344\n",
            "new max len 2600 10 35911 2393.0952\n",
            "new max len 2700 10 37289 1510.8402\n",
            "new max len 2800 10 38709 1041.2988\n",
            "new max len 2900 10 40076 306.6561\n",
            "new max len 3000 10 41332 36.252975\n",
            "new max len 3100 10 42542 136.43538\n",
            "new max len 3200 10 43733 88.20502\n",
            "new max len 3300 10 44917 153.15645\n",
            "new max len 3400 10 46110 261.32785\n",
            "new max len 3500 10 47316 371.339\n",
            "new max len 3600 10 48532 386.49713\n",
            "new max len 3700 10 49768 678.2637\n",
            "new max len 3800 10 51032 243.70973\n",
            "new max len 3900 10 52289 40.895008\n",
            "new max len 4000 10 53614 88.310814\n",
            "new max len 4100 10 54933 254.13768\n",
            "new max len 4200 10 56203 1125.6602\n",
            "new max len 4300 10 57537 1311.637\n",
            "new max len 4400 10 58950 485.68524\n",
            "new max len 4500 10 60273 381.0782\n",
            "new max len 4600 10 61509 215.7952\n",
            "new max len 4700 10 62743 487.41425\n",
            "new max len 4800 10 64101 401.46072\n",
            "new max len 4900 10 65440 170.5453\n",
            "new max len 5000 10 66679 206.6702\n",
            "new max len 5100 10 67881 323.35052\n",
            "new max len 5200 10 69084 574.0836\n",
            "new max len 5300 10 70411 880.7818\n",
            "new max len 5400 10 71745 847.27203\n",
            "new max len 5500 10 72980 213.92201\n",
            "new max len 5600 10 74289 472.62726\n",
            "new max len 5700 10 75636 913.41846\n",
            "new max len 5800 10 77011 344.76233\n",
            "new max len 5900 10 78363 291.54294\n",
            "new max len 6000 10 79605 101.34273\n",
            "new max len 6100 10 80882 275.31766\n",
            "new max len 6200 10 82133 94.466965\n",
            "new max len 6300 10 83454 444.57544\n",
            "new max len 6400 10 84645 87.47784\n",
            "new max len 6500 10 85916 274.14532\n",
            "new max len 6600 10 87109 814.3984\n",
            "new max len 6700 10 88326 1006.2346\n",
            "new max len 6800 10 89588 983.50916\n",
            "new max len 6900 10 90827 1032.8308\n",
            "new max len 7000 10 92128 1919.8496\n",
            "new max len 7100 10 93464 2193.7512\n",
            "new max len 7200 10 94709 2017.4231\n",
            "new max len 7300 10 96133 917.97614\n",
            "new max len 7400 10 97510 432.49957\n",
            "new max len 7500 10 98873 159.18019\n",
            "new max len 7600 10 100067 112.79194\n",
            "new max len 7700 10 101235 219.78716\n",
            "new max len 7800 10 102516 97.658806\n",
            "new max len 7900 10 103721 321.71667\n",
            "new max len 8000 10 104943 658.23944\n",
            "new max len 8100 10 106183 762.64514\n",
            "new max len 8200 10 107481 1160.712\n",
            "new max len 8300 10 108753 1820.9847\n",
            "new max len 8400 10 110074 1739.3069\n",
            "new max len 8500 10 111478 982.1148\n",
            "new max len 8600 10 112875 541.01294\n",
            "new max len 8700 10 114157 215.26666\n",
            "new max len 8800 10 115419 233.5966\n",
            "new max len 8900 10 116694 640.39844\n",
            "new max len 9000 10 117876 742.2635\n",
            "new max len 9100 10 119085 842.00635\n",
            "new max len 9200 10 120352 645.65015\n",
            "new max len 9300 10 121611 1611.6948\n",
            "new max len 9400 10 122897 1860.1655\n",
            "new max len 9500 10 124314 1148.7407\n",
            "new max len 9600 10 125641 223.5533\n",
            "new max len 9700 10 126922 46.60964\n",
            "new max len 9800 10 128168 55.868774\n",
            "new max len 9900 10 129428 141.55493\n",
            "new max len 10000 10 130643 286.0681\n",
            "new max len 10100 10 131939 409.80142\n",
            "new max len 10200 10 133183 340.95935\n",
            "new max len 10300 10 134378 516.063\n",
            "new max len 10400 10 135623 384.37613\n",
            "new max len 10500 10 136976 981.96875\n",
            "new max len 10600 10 138363 283.57343\n",
            "new max len 10700 10 139819 318.26892\n",
            "new max len 10800 10 141185 407.3933\n",
            "new max len 10900 10 142470 795.7984\n",
            "new max len 11000 10 143743 926.94415\n",
            "new max len 11100 10 144995 923.4078\n",
            "new max len 11200 10 146262 988.6806\n",
            "new max len 11300 10 147581 1284.6943\n",
            "new max len 11400 10 148908 1755.6069\n",
            "new max len 11500 10 150271 1615.4437\n",
            "new max len 11600 10 151634 1965.3513\n",
            "new max len 11700 10 153009 1853.1875\n",
            "new max len 11800 10 154331 2379.1802\n",
            "new max len 11900 10 155656 2064.7937\n",
            "new max len 12000 10 156958 3764.9333\n",
            "new max len 12100 10 158206 5315.3\n",
            "new max len 12200 10 159473 5894.3433\n",
            "new max len 12300 10 160746 6346.08\n",
            "new max len 12400 10 161997 5040.236\n",
            "new max len 12500 10 163398 3358.3577\n",
            "new max len 12600 10 164789 3820.4834\n",
            "new max len 12700 10 166161 4234.628\n",
            "new max len 12800 10 167585 2768.7454\n",
            "new max len 12900 10 168989 1384.794\n",
            "new max len 13000 10 170450 700.0296\n",
            "new max len 13100 10 171767 402.64728\n",
            "new max len 13200 10 173014 61.59536\n",
            "new max len 13300 10 174335 149.07748\n",
            "new max len 13400 10 175527 163.49184\n",
            "new max len 13500 10 176943 199.22684\n",
            "new max len 13600 10 178290 464.68387\n",
            "new max len 13700 10 179536 309.53122\n",
            "new max len 13800 10 180775 515.75104\n",
            "new max len 13900 10 182059 820.11804\n",
            "new max len 14000 10 183315 486.30545\n",
            "new max len 14100 10 184568 831.77075\n",
            "new max len 14200 10 185947 196.70619\n",
            "new max len 14300 10 187386 34.453033\n",
            "new max len 14400 10 188581 124.05753\n",
            "new max len 14500 10 189817 212.58589\n",
            "new max len 14600 10 191005 293.7174\n",
            "new max len 14700 10 192309 398.51984\n",
            "new max len 14800 10 193591 140.22429\n",
            "new max len 14900 10 194789 170.4936\n",
            "new max len 15000 10 196094 1013.7241\n",
            "new max len 15100 10 197436 1000.72595\n",
            "new max len 15200 10 198796 496.68365\n",
            "new max len 15300 10 200068 373.00137\n",
            "new max len 15400 10 201420 392.88336\n",
            "new max len 15500 10 202786 685.2195\n",
            "new max len 15600 10 204123 737.22546\n",
            "new max len 15700 10 205434 1088.9565\n",
            "new max len 15800 10 206762 1673.0471\n",
            "new max len 15900 10 208063 1934.519\n",
            "new max len 16000 10 209374 2053.2969\n",
            "new max len 16100 10 210757 1222.2412\n",
            "new max len 16200 10 212089 680.97473\n",
            "new max len 16300 10 213423 562.8883\n",
            "new max len 16400 10 214748 349.15985\n",
            "new max len 16500 10 216030 523.43835\n",
            "new max len 16600 10 217373 334.1439\n",
            "new max len 16700 10 218696 554.91113\n",
            "new max len 16800 10 220024 595.89307\n",
            "new max len 16900 10 221281 851.9983\n",
            "new max len 17000 10 222622 968.9331\n",
            "new max len 17100 10 223903 1344.1357\n",
            "new max len 17200 10 225209 1465.635\n",
            "new max len 17300 10 226651 528.7509\n",
            "new max len 17400 10 228054 103.06005\n",
            "new max len 17500 10 229230 184.68959\n",
            "new max len 17600 10 230481 209.10289\n",
            "new max len 17700 10 231816 478.892\n",
            "new max len 17800 10 233100 226.54655\n",
            "new max len 17900 10 234336 160.22916\n",
            "new max len 18000 10 235588 463.47394\n",
            "new max len 18100 10 236824 551.8758\n",
            "new max len 18200 10 238052 386.6964\n",
            "new max len 18300 10 239250 1109.2505\n",
            "new max len 18400 10 240522 1400.9221\n",
            "new max len 18500 10 241792 1039.6763\n",
            "new max len 18600 10 243089 1584.7664\n",
            "new max len 18700 10 244376 1137.6105\n",
            "new max len 18800 10 245748 578.34015\n",
            "new max len 18900 10 247151 269.06146\n",
            "new max len 19000 10 248494 44.05315\n",
            "new max len 19100 10 249642 56.005695\n",
            "new max len 19200 10 250813 243.46053\n",
            "new max len 19300 10 252089 336.4702\n",
            "new max len 19400 10 253336 647.87646\n",
            "new max len 19500 10 254623 290.0952\n",
            "new max len 19600 10 255830 69.36828\n",
            "new max len 19700 10 257150 126.88268\n",
            "new max len 19800 10 258414 395.65247\n",
            "new max len 19900 10 259596 715.7469\n",
            "new max len 20000 10 260864 451.43753\n",
            "new max len 20100 10 262129 430.50635\n",
            "new max len 20200 10 263482 1017.4407\n",
            "new max len 20300 10 264855 411.938\n",
            "new max len 20400 10 266170 246.0999\n",
            "new max len 20500 10 267523 258.4582\n",
            "new max len 20600 10 268841 242.00218\n",
            "new max len 20700 10 270105 372.63745\n",
            "new max len 20800 10 271368 150.85045\n",
            "new max len 20900 10 272585 758.8123\n",
            "new max len 21000 10 273844 1688.2334\n",
            "new max len 21100 10 275183 1575.0481\n",
            "new max len 21200 10 276594 634.97784\n",
            "new max len 21300 10 277883 39.629196\n",
            "Кратчайший путь: 21322\n",
            "['X', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 100  # Ширина \"луча\"\n",
        "model.eval();\n",
        "path = beam_search_sorting(permutation, beam_width=k)\n",
        "if path:\n",
        "  print(\"Кратчайший путь:\", len(path))\n",
        "  print(path)\n",
        "else:\n",
        "  print('No path found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "2DTOLzkUaM0Q",
        "outputId": "c1d2c7e8-8d87-4196-b279-c04ec03a8b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new max len 100 100 16628 6860.6694\n",
            "new max len 200 100 32786 6867.855\n",
            "new max len 300 100 47828 6592.356\n",
            "new max len 400 100 61995 6379.0015\n",
            "new max len 500 100 76290 5533.2944\n",
            "new max len 600 100 91544 5528.895\n",
            "new max len 700 100 107269 4944.6616\n",
            "new max len 800 100 122454 3759.0889\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-984ea41efb67>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m  \u001b[0;31m# Ширина \"луча\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search_sorting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Кратчайший путь:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-163c520bdad1>\u001b[0m in \u001b[0;36mbeam_search_sorting\u001b[0;34m(permutation, beam_width)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_predictions_on_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;31m# q_value = model(t_state).squeeze().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                         \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-67394bdee2d8>\u001b[0m in \u001b[0;36mmake_predictions_on_array\u001b[0;34m(model, array)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_X\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-e24692e2fb7a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# 4) Pass through each gated TCN block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# 5) Global pool => (batch, hidden_dim, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-e24692e2fb7a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Second conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n\u001b[0;32m--> 370\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 32496 -> 21322"
      ],
      "metadata": {
        "id": "uw3JcPX7fBog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = GatedTCN1D(vocab_size=dimension, embed_dim=64, hidden_dim=256,\n",
        "                       num_levels=6, kernel_size=3, dropout=0.3, out_dim=1).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.8, verbose=True)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000\n",
        "print(f'Total trainable parameters: {pytorch_total_params}')\n",
        "model, train_rmse_list, train_r2_list, test_rmse_list, test_r2_list, loss_list = train(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0sX2GY3aNfy",
        "outputId": "218b50e4-7367-4309-bfe4-985774095342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 3.572225\n",
            "X.shape=torch.Size([9000000, 40])\n",
            "Epoch [1/100], Loss: 236392.3750\n",
            "Train - RMSE: 512.7828, R²: 0.9610\n",
            "Test  - RMSE: 1439.8027, R²: 0.6929\n",
            "Epoch [2/100], Loss: 110769.2344\n",
            "Train - RMSE: 306.8331, R²: 0.9861\n",
            "Test  - RMSE: 1154.5050, R²: 0.8025\n",
            "Epoch [3/100], Loss: 133935.4062\n",
            "Train - RMSE: 273.0778, R²: 0.9890\n",
            "Test  - RMSE: 1160.4948, R²: 0.8005\n",
            "Epoch [4/100], Loss: 245607.5625\n",
            "Train - RMSE: 125.7377, R²: 0.9977\n",
            "Test  - RMSE: 1052.6852, R²: 0.8358\n",
            "Epoch [5/100], Loss: 86071.2031\n",
            "Train - RMSE: 140.8219, R²: 0.9971\n",
            "Test  - RMSE: 1066.4022, R²: 0.8315\n",
            "Epoch [6/100], Loss: 38557.3828\n",
            "Train - RMSE: 166.7094, R²: 0.9959\n",
            "Test  - RMSE: 1071.3007, R²: 0.8300\n",
            "Epoch [7/100], Loss: 44742.1445\n",
            "Train - RMSE: 161.2146, R²: 0.9961\n",
            "Test  - RMSE: 1060.5569, R²: 0.8334\n",
            "Epoch [8/100], Loss: 66082.3906\n",
            "Train - RMSE: 184.9591, R²: 0.9949\n",
            "Test  - RMSE: 1069.3910, R²: 0.8306\n",
            "Epoch [9/100], Loss: 56821.6172\n",
            "Train - RMSE: 212.3311, R²: 0.9933\n",
            "Test  - RMSE: 1087.2189, R²: 0.8249\n",
            "Early stopping triggered\n",
            "Finished Training\n",
            "Best metrics: min(test_rmse_list)=1052.6852 max(test_r2_list)=0.8358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10  # Ширина \"луча\"\n",
        "model.eval();\n",
        "path = beam_search_sorting(permutation, beam_width=k)\n",
        "if path:\n",
        "  print(\"Кратчайший путь:\", len(path))\n",
        "  print(path)\n",
        "else:\n",
        "  print('No path found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNeo25zfKDB9",
        "outputId": "a41d94fe-8441-43c0-c181-d94bcf5c8e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new max len 100 10 1799 8215.915\n",
            "new max len 200 10 3516 7810.1367\n",
            "new max len 300 10 5133 7484.3584\n",
            "new max len 400 10 6692 7276.143\n",
            "new max len 500 10 8110 6937.1484\n",
            "new max len 600 10 9622 6068.4307\n",
            "new max len 700 10 11230 5391.6074\n",
            "new max len 800 10 12763 4407.8887\n",
            "new max len 900 10 14306 4307.674\n",
            "new max len 1000 10 15767 3544.4795\n",
            "new max len 1100 10 17272 1985.5748\n",
            "new max len 1200 10 18718 953.8212\n",
            "new max len 1300 10 20084 396.62567\n",
            "new max len 1400 10 21427 89.65303\n",
            "new max len 1500 10 22667 61.10315\n",
            "new max len 1600 10 23916 164.61758\n",
            "new max len 1700 10 25163 316.60645\n",
            "new max len 1800 10 26446 168.43951\n",
            "new max len 1900 10 27634 115.68915\n",
            "new max len 2000 10 28864 203.70316\n",
            "new max len 2100 10 30082 86.825325\n",
            "new max len 2200 10 31283 150.45686\n",
            "new max len 2300 10 32510 181.21222\n",
            "new max len 2400 10 33760 242.73027\n",
            "new max len 2500 10 34997 156.04831\n",
            "new max len 2600 10 36192 323.12457\n",
            "new max len 2700 10 37408 393.46118\n",
            "new max len 2800 10 38669 289.92236\n",
            "new max len 2900 10 39901 350.72888\n",
            "new max len 3000 10 41197 375.41214\n",
            "new max len 3100 10 42511 241.00357\n",
            "new max len 3200 10 43789 97.21486\n",
            "new max len 3300 10 45050 77.53307\n",
            "new max len 3400 10 46295 213.83554\n",
            "new max len 3500 10 47516 318.88385\n",
            "new max len 3600 10 48841 150.08682\n",
            "new max len 3700 10 50205 421.2723\n",
            "new max len 3800 10 51570 304.0374\n",
            "new max len 3900 10 52848 195.77644\n",
            "new max len 4000 10 54070 100.15085\n",
            "new max len 4100 10 55295 87.10351\n",
            "new max len 4200 10 56550 156.184\n",
            "new max len 4300 10 57777 120.85445\n",
            "new max len 4400 10 59021 357.67105\n",
            "new max len 4500 10 60238 849.9437\n",
            "new max len 4600 10 61515 909.60364\n",
            "new max len 4700 10 62871 1526.0323\n",
            "new max len 4800 10 64229 1521.836\n",
            "new max len 4900 10 65561 2518.4092\n",
            "new max len 5000 10 66869 2941.3206\n",
            "new max len 5100 10 68184 3424.217\n",
            "new max len 5200 10 69477 2927.6365\n",
            "new max len 5300 10 70777 2036.9694\n",
            "new max len 5400 10 72135 2121.3088\n",
            "new max len 5500 10 73531 1937.9065\n",
            "new max len 5600 10 74883 2514.9333\n",
            "new max len 5700 10 76283 2506.3757\n",
            "new max len 5800 10 77584 3181.0598\n",
            "new max len 5900 10 78873 3220.5085\n",
            "new max len 6000 10 80106 4291.701\n",
            "new max len 6100 10 81343 4722.6465\n",
            "new max len 6200 10 82622 4840.121\n",
            "new max len 6300 10 83937 3399.9387\n",
            "new max len 6400 10 85385 1539.4797\n",
            "new max len 6500 10 86766 375.60748\n",
            "new max len 6600 10 88051 47.576885\n",
            "new max len 6700 10 89198 125.95396\n",
            "new max len 6800 10 90393 79.606865\n",
            "new max len 6900 10 91631 46.46079\n",
            "new max len 7000 10 92862 170.20398\n",
            "new max len 7100 10 94105 512.48126\n",
            "new max len 7200 10 95356 716.9228\n",
            "new max len 7300 10 96652 873.68286\n",
            "new max len 7400 10 97989 890.0752\n",
            "new max len 7500 10 99320 1099.9791\n",
            "new max len 7600 10 100660 1136.4192\n",
            "new max len 7700 10 102021 1863.1002\n",
            "new max len 7800 10 103379 1524.1155\n",
            "new max len 7900 10 104716 1996.7446\n",
            "new max len 8000 10 106170 1087.692\n",
            "new max len 8100 10 107584 1237.0446\n",
            "new max len 8200 10 109019 1605.933\n",
            "new max len 8300 10 110450 1640.9554\n",
            "new max len 8400 10 111846 1408.5752\n",
            "new max len 8500 10 113205 1483.6642\n",
            "new max len 8600 10 114562 2835.9211\n",
            "new max len 8700 10 115921 3558.3616\n",
            "new max len 8800 10 117333 2541.8796\n",
            "new max len 8900 10 118816 1293.9989\n",
            "new max len 9000 10 120171 549.00916\n",
            "new max len 9100 10 121417 353.79626\n",
            "new max len 9200 10 122627 259.61407\n",
            "new max len 9300 10 123873 279.73584\n",
            "new max len 9400 10 125162 53.52805\n",
            "new max len 9500 10 126511 80.78567\n",
            "new max len 9600 10 127704 162.33063\n",
            "new max len 9700 10 128929 192.46066\n",
            "new max len 9800 10 130222 282.7595\n",
            "new max len 9900 10 131570 551.447\n",
            "new max len 10000 10 132957 159.20891\n",
            "new max len 10100 10 134300 48.862286\n",
            "new max len 10200 10 135493 107.3577\n",
            "new max len 10300 10 136690 153.19138\n",
            "new max len 10400 10 137898 219.85251\n",
            "new max len 10500 10 139180 192.46655\n",
            "new max len 10600 10 140381 210.12903\n",
            "new max len 10700 10 141603 207.4871\n",
            "new max len 10800 10 142915 67.1722\n",
            "new max len 10900 10 144105 164.13724\n",
            "new max len 11000 10 145359 75.47903\n",
            "new max len 11100 10 146700 201.58539\n",
            "new max len 11200 10 148067 359.85025\n",
            "new max len 11300 10 149348 189.5448\n",
            "new max len 11400 10 150574 110.3392\n",
            "new max len 11500 10 151829 177.83119\n",
            "new max len 11600 10 153085 154.62433\n",
            "new max len 11700 10 154300 154.28772\n",
            "new max len 11800 10 155503 117.590096\n",
            "new max len 11900 10 156690 80.05328\n",
            "new max len 12000 10 157917 58.641747\n",
            "new max len 12100 10 159193 164.1538\n",
            "new max len 12200 10 160401 408.19513\n",
            "new max len 12300 10 161622 465.25525\n",
            "new max len 12400 10 162866 497.48895\n",
            "new max len 12500 10 164092 662.5165\n",
            "new max len 12600 10 165328 802.8811\n",
            "new max len 12700 10 166580 1292.2891\n",
            "new max len 12800 10 167972 1454.0182\n",
            "new max len 12900 10 169378 1977.215\n",
            "new max len 13000 10 170756 2424.4856\n",
            "new max len 13100 10 172120 3394.8728\n",
            "new max len 13200 10 173500 4257.759\n",
            "new max len 13300 10 174914 4623.3223\n",
            "new max len 13400 10 176305 3503.8105\n",
            "new max len 13500 10 177741 2918.6387\n",
            "new max len 13600 10 179200 2778.3545\n",
            "new max len 13700 10 180629 3335.9788\n",
            "new max len 13800 10 182021 4807.0205\n",
            "new max len 13900 10 183355 3968.512\n",
            "new max len 14000 10 184723 2484.0193\n",
            "new max len 14100 10 186184 1779.9437\n",
            "new max len 14200 10 187563 964.09875\n",
            "new max len 14300 10 188929 900.48303\n",
            "new max len 14400 10 190282 181.62775\n",
            "new max len 14500 6 191471 60.55281\n",
            "No path found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 20  # Ширина \"луча\"\n",
        "model.eval();\n",
        "path = beam_search_sorting(permutation, beam_width=k)\n",
        "if path:\n",
        "  print(\"Кратчайший путь:\", len(path))\n",
        "  print(path)\n",
        "else:\n",
        "  print('No path found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIkhYcIqfERD",
        "outputId": "92a04653-b23b-4cc8-fe13-40a6cfb5bb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new max len 100 20 3486 8082.0635\n",
            "new max len 200 20 6669 7899.4937\n",
            "new max len 300 20 9919 7682.0117\n",
            "new max len 400 20 13022 7327.7134\n",
            "new max len 500 20 15966 6892.7847\n",
            "new max len 600 20 19114 6124.3315\n",
            "new max len 700 20 22362 5609.128\n",
            "new max len 800 20 25452 5272.619\n",
            "new max len 900 20 28475 3171.8206\n",
            "new max len 1000 20 31501 1794.8247\n",
            "new max len 1100 20 34493 816.8391\n",
            "new max len 1200 20 37464 132.63293\n",
            "new max len 1300 20 39949 52.83428\n",
            "new max len 1400 20 42277 109.14501\n",
            "new max len 1500 20 44715 55.21481\n",
            "new max len 1600 20 47152 176.7174\n",
            "new max len 1700 20 49622 256.03354\n",
            "new max len 1800 20 52088 432.80756\n",
            "new max len 1900 20 54549 517.4871\n",
            "new max len 2000 20 57094 360.45673\n",
            "new max len 2100 20 59566 196.39426\n",
            "new max len 2200 20 62025 153.62598\n",
            "new max len 2300 20 64445 111.65724\n",
            "new max len 2400 20 66954 198.15662\n",
            "new max len 2500 20 69422 162.65097\n",
            "new max len 2600 20 72070 316.70905\n",
            "new max len 2700 20 74721 179.25566\n",
            "new max len 2800 20 77383 103.07468\n",
            "new max len 2900 20 79932 234.35156\n",
            "new max len 3000 20 82486 299.2379\n",
            "new max len 3100 20 85123 430.33698\n",
            "new max len 3200 20 87677 759.1074\n",
            "new max len 3300 20 90255 963.21155\n",
            "new max len 3400 20 92901 1508.8605\n",
            "new max len 3500 20 95514 1897.7654\n",
            "new max len 3600 20 98124 2656.048\n",
            "new max len 3700 20 100792 1879.9987\n",
            "new max len 3800 20 103493 1630.545\n",
            "new max len 3900 20 106043 2336.7458\n",
            "new max len 4000 20 108648 1785.8163\n",
            "new max len 4100 20 111444 999.11426\n",
            "new max len 4200 20 114089 416.76294\n",
            "new max len 4300 20 116903 208.24121\n",
            "new max len 4400 20 119419 123.2708\n",
            "new max len 4500 20 121924 91.87328\n",
            "new max len 4600 20 124317 95.579056\n",
            "new max len 4700 20 126757 58.877064\n",
            "new max len 4800 20 129163 95.64768\n",
            "new max len 4900 20 131681 110.137344\n",
            "new max len 5000 20 134188 133.05307\n",
            "new max len 5100 20 136511 296.99405\n",
            "new max len 5200 20 138938 440.0438\n",
            "new max len 5300 20 141407 395.30875\n",
            "new max len 5400 20 143889 315.77545\n",
            "new max len 5500 20 146400 330.0105\n",
            "new max len 5600 20 148960 227.86615\n",
            "new max len 5700 20 151456 235.53029\n",
            "new max len 5800 20 154123 57.358746\n",
            "new max len 5900 20 156560 228.67773\n",
            "new max len 6000 20 159138 491.77994\n",
            "new max len 6100 20 161659 702.3652\n",
            "new max len 6200 20 164140 682.8552\n",
            "new max len 6300 20 166718 557.65564\n",
            "new max len 6400 20 169230 375.7353\n",
            "new max len 6500 20 171759 451.29108\n",
            "new max len 6600 20 174253 679.8554\n",
            "new max len 6700 20 176827 186.66232\n",
            "new max len 6800 20 179268 247.91556\n",
            "new max len 6900 20 181815 78.257614\n",
            "new max len 7000 20 184280 130.41724\n",
            "new max len 7100 20 186762 375.68863\n",
            "new max len 7200 20 189342 550.48425\n",
            "new max len 7300 20 191891 589.1457\n",
            "new max len 7400 20 194525 806.74854\n",
            "new max len 7500 20 197328 758.57806\n",
            "new max len 7600 20 200007 993.146\n",
            "new max len 7700 20 202729 1229.0665\n",
            "new max len 7800 20 205393 975.9634\n",
            "new max len 7900 20 208002 851.0793\n",
            "new max len 8000 20 210668 995.0194\n",
            "new max len 8100 20 213294 1331.106\n",
            "new max len 8200 20 215962 1406.6993\n",
            "new max len 8300 20 218569 1532.5504\n",
            "new max len 8400 20 221112 1774.5413\n",
            "new max len 8500 20 223863 1227.0433\n",
            "new max len 8600 20 226713 719.19324\n",
            "new max len 8700 20 229463 649.2554\n",
            "new max len 8800 20 232001 153.20561\n",
            "new max len 8900 20 234460 165.74026\n",
            "new max len 9000 20 236874 130.90065\n",
            "new max len 9100 20 239293 84.753006\n",
            "new max len 9200 20 241863 95.44492\n",
            "new max len 9300 20 244260 143.27371\n",
            "new max len 9400 20 246706 227.49115\n",
            "new max len 9500 20 249137 103.68064\n",
            "new max len 9600 20 251603 92.32985\n",
            "new max len 9700 20 254027 178.23956\n",
            "new max len 9800 20 256467 68.29837\n",
            "new max len 9900 20 258868 52.55831\n",
            "new max len 10000 20 261232 73.858154\n",
            "new max len 10100 20 263675 108.19253\n",
            "new max len 10200 20 266138 73.69478\n",
            "new max len 10300 20 268617 186.35419\n",
            "new max len 10400 20 271043 66.84483\n",
            "new max len 10500 20 273391 103.22063\n",
            "new max len 10600 20 275972 87.31575\n",
            "new max len 10700 20 278451 174.742\n",
            "new max len 10800 20 280913 276.10242\n",
            "new max len 10900 20 283421 72.58976\n",
            "new max len 11000 20 285866 136.49799\n",
            "new max len 11100 20 288289 87.8049\n",
            "new max len 11200 20 290674 87.64862\n",
            "new max len 11300 20 293037 185.02982\n",
            "new max len 11400 20 295433 269.3536\n",
            "new max len 11500 20 297961 275.53384\n",
            "new max len 11600 20 300374 66.508514\n",
            "new max len 11700 20 303064 62.040066\n",
            "new max len 11800 20 305558 149.78171\n",
            "new max len 11900 20 308075 90.37273\n",
            "new max len 12000 20 310478 91.07938\n",
            "new max len 12100 20 312916 181.28564\n",
            "new max len 12200 20 315309 76.171616\n",
            "new max len 12300 20 317685 93.33551\n",
            "new max len 12400 20 320207 62.45468\n",
            "new max len 12500 20 322620 114.52496\n",
            "new max len 12600 20 325071 77.9861\n",
            "new max len 12700 20 327476 73.05475\n",
            "new max len 12800 20 329849 79.31765\n",
            "new max len 12900 20 332304 149.69969\n",
            "new max len 13000 20 334862 113.29502\n",
            "new max len 13100 20 337328 126.79045\n",
            "new max len 13200 20 339894 224.31447\n",
            "new max len 13300 20 342427 75.99418\n",
            "new max len 13400 20 345002 170.30258\n",
            "new max len 13500 20 347515 263.43628\n",
            "new max len 13600 20 350058 122.75328\n",
            "new max len 13700 20 352664 103.85445\n",
            "new max len 13800 20 355222 147.17538\n",
            "new max len 13900 20 357545 99.694336\n",
            "new max len 14000 20 360035 90.333405\n",
            "new max len 14100 20 362455 105.22211\n",
            "new max len 14200 20 364893 103.93066\n",
            "new max len 14300 20 367242 159.39857\n",
            "new max len 14400 20 369629 203.9769\n",
            "new max len 14500 20 372052 69.36598\n",
            "new max len 14600 20 374489 93.41224\n",
            "new max len 14700 20 376946 92.579445\n",
            "new max len 14800 20 379385 61.33186\n",
            "new max len 14900 20 381861 63.668507\n",
            "new max len 15000 20 384214 132.32175\n",
            "new max len 15100 20 386567 68.86013\n",
            "new max len 15200 20 388909 101.70145\n",
            "new max len 15300 20 391310 155.47372\n",
            "new max len 15400 20 393916 99.00609\n",
            "new max len 15500 20 396472 205.4769\n",
            "new max len 15600 20 398992 374.63208\n",
            "new max len 15700 20 401533 231.24425\n",
            "new max len 15800 20 404079 80.50883\n",
            "new max len 15900 20 406638 81.147285\n",
            "new max len 16000 20 409206 195.81633\n",
            "new max len 16100 20 411819 274.46222\n",
            "new max len 16200 20 414379 392.80316\n",
            "new max len 16300 20 416964 472.1707\n",
            "new max len 16400 20 419563 945.68616\n",
            "new max len 16500 20 422251 699.74915\n",
            "new max len 16600 20 424866 192.50217\n",
            "new max len 16700 20 427206 93.05542\n",
            "new max len 16800 20 429614 99.93663\n",
            "new max len 16900 20 432081 80.67172\n",
            "new max len 17000 20 434507 102.1834\n",
            "new max len 17100 20 436819 64.42847\n",
            "new max len 17200 20 439327 126.77211\n",
            "new max len 17300 20 441782 181.55103\n",
            "new max len 17400 20 444289 131.73181\n",
            "new max len 17500 20 446791 354.21277\n",
            "new max len 17600 20 449253 469.75177\n",
            "new max len 17700 20 451725 549.2978\n",
            "new max len 17800 20 454317 528.84894\n",
            "new max len 17900 20 456757 425.96378\n",
            "new max len 18000 20 459256 275.1421\n",
            "new max len 18100 20 461733 623.2539\n",
            "new max len 18200 20 464180 1012.25085\n",
            "new max len 18300 20 466744 1199.8895\n",
            "new max len 18400 20 469354 1247.5172\n",
            "new max len 18500 20 471950 706.2177\n",
            "new max len 18600 20 474704 193.10767\n",
            "new max len 18700 20 477340 139.75865\n",
            "new max len 18800 20 479653 61.742023\n",
            "new max len 18900 20 482071 43.732353\n",
            "new max len 19000 20 484305 76.69993\n",
            "new max len 19100 20 486647 170.35579\n",
            "new max len 19200 20 489174 61.91598\n",
            "new max len 19300 20 491680 51.636814\n",
            "new max len 19400 20 494102 90.490814\n",
            "new max len 19500 20 496621 78.585075\n",
            "new max len 19600 20 499018 87.38156\n",
            "new max len 19700 20 501506 62.049515\n",
            "new max len 19800 20 503945 157.25063\n",
            "new max len 19900 20 506424 281.03055\n",
            "new max len 20000 20 508910 406.98746\n",
            "new max len 20100 20 511346 537.6871\n",
            "new max len 20200 20 513836 263.92462\n",
            "new max len 20300 20 516365 164.23584\n",
            "new max len 20400 20 518905 87.17366\n",
            "new max len 20500 20 521395 58.155304\n",
            "new max len 20600 20 523719 106.27912\n",
            "new max len 20700 20 526151 150.77307\n",
            "new max len 20800 20 528623 84.86624\n",
            "new max len 20900 20 531113 67.91481\n",
            "new max len 21000 20 533672 100.565\n",
            "new max len 21100 20 536091 86.46588\n",
            "new max len 21200 20 538588 199.36313\n",
            "new max len 21300 20 541089 141.19324\n",
            "new max len 21400 20 543610 405.8261\n",
            "new max len 21500 20 546203 599.7782\n",
            "new max len 21600 20 548726 876.7638\n",
            "new max len 21700 20 551243 1142.9515\n",
            "new max len 21800 20 553890 1716.5157\n",
            "new max len 21900 20 556464 3109.5684\n",
            "new max len 22000 20 559064 2879.6074\n",
            "new max len 22100 20 561800 1866.8042\n",
            "new max len 22200 20 564548 880.8163\n",
            "new max len 22300 20 567298 128.35281\n",
            "new max len 22400 20 569683 43.232174\n",
            "new max len 22500 20 572052 46.807377\n",
            "new max len 22600 20 574266 93.45694\n",
            "new max len 22700 20 576686 95.81828\n",
            "new max len 22800 20 579234 73.81802\n",
            "new max len 22900 20 581673 81.15871\n",
            "new max len 23000 20 584240 201.43959\n",
            "new max len 23100 20 586639 107.73476\n",
            "new max len 23200 20 589043 206.42407\n",
            "new max len 23300 20 591550 183.53156\n",
            "new max len 23400 20 594064 241.07788\n",
            "new max len 23500 20 596540 133.54468\n",
            "new max len 23600 20 598965 180.55981\n",
            "new max len 23700 20 601451 187.93431\n",
            "new max len 23800 20 603970 199.70416\n",
            "new max len 23900 20 606598 260.25787\n",
            "new max len 24000 20 609081 425.67456\n",
            "new max len 24100 20 611488 161.24162\n",
            "new max len 24200 20 613868 330.88416\n",
            "new max len 24300 20 616326 315.08978\n",
            "new max len 24400 20 618711 148.95105\n",
            "new max len 24500 20 621212 316.15784\n",
            "new max len 24600 20 623638 683.0305\n",
            "new max len 24700 20 626142 865.2162\n",
            "new max len 24800 20 628698 1171.7368\n",
            "new max len 24900 20 631319 1786.4526\n",
            "new max len 25000 20 633988 1912.1406\n",
            "new max len 25100 20 636682 1774.5265\n",
            "new max len 25200 20 639511 1506.9581\n",
            "new max len 25300 20 642193 1814.5543\n",
            "new max len 25400 20 645016 770.7763\n",
            "new max len 25500 20 647618 437.53503\n",
            "new max len 25600 20 650169 212.03807\n",
            "new max len 25700 20 652567 145.63416\n",
            "new max len 25800 20 654928 67.82939\n",
            "new max len 25900 20 657178 96.89885\n",
            "new max len 26000 20 659560 84.362175\n",
            "new max len 26100 20 661922 127.56411\n",
            "new max len 26200 20 664419 118.38437\n",
            "new max len 26300 20 666816 104.984344\n",
            "new max len 26400 20 669182 168.71225\n",
            "new max len 26500 20 671638 150.15231\n",
            "new max len 26600 20 674008 58.58682\n",
            "new max len 26700 20 676347 79.54881\n",
            "new max len 26800 20 678706 72.840515\n",
            "new max len 26900 20 680999 164.17355\n",
            "new max len 27000 20 683440 92.26777\n",
            "new max len 27100 20 685864 45.04352\n",
            "new max len 27200 20 688240 108.62411\n",
            "new max len 27300 20 690685 63.888046\n",
            "new max len 27400 20 693065 61.9605\n",
            "new max len 27500 20 695461 72.65869\n",
            "new max len 27600 20 697787 153.70447\n",
            "new max len 27700 20 700178 246.90582\n",
            "new max len 27800 20 702640 182.19582\n",
            "new max len 27900 20 705045 240.04149\n",
            "new max len 28000 20 707527 189.11505\n",
            "new max len 28100 20 710051 82.574394\n",
            "new max len 28200 20 712673 130.477\n",
            "new max len 28300 20 715141 118.917114\n",
            "new max len 28400 20 717706 181.93953\n",
            "new max len 28500 20 720246 456.8107\n",
            "new max len 28600 20 722751 508.94858\n",
            "new max len 28700 20 725395 248.36424\n",
            "new max len 28800 20 728107 462.11874\n",
            "new max len 28900 20 730648 512.79\n",
            "new max len 29000 20 733264 591.89417\n",
            "new max len 29100 20 735786 525.61365\n",
            "new max len 29200 20 738310 504.92307\n",
            "new max len 29300 20 740804 236.71037\n",
            "new max len 29400 20 743212 110.375496\n",
            "new max len 29500 20 745564 86.92628\n",
            "new max len 29600 20 747867 153.34995\n",
            "new max len 29700 20 750267 56.299866\n",
            "new max len 29800 20 752645 99.829346\n",
            "new max len 29900 20 755126 65.5767\n",
            "new max len 30000 20 757477 113.83372\n",
            "new max len 30100 20 759982 83.44014\n",
            "new max len 30200 20 762554 91.08246\n",
            "new max len 30300 20 765027 80.16261\n",
            "new max len 30400 20 767483 86.61113\n",
            "new max len 30500 20 769954 95.27823\n",
            "new max len 30600 20 772484 133.94188\n",
            "new max len 30700 20 775041 265.90314\n",
            "new max len 30800 20 777656 176.89935\n",
            "new max len 30900 20 780103 126.60059\n",
            "new max len 31000 20 782699 75.680695\n",
            "new max len 31100 20 785122 107.53885\n",
            "new max len 31200 20 787600 180.68115\n",
            "new max len 31300 20 790136 150.70184\n",
            "new max len 31400 20 792688 198.30417\n",
            "new max len 31500 20 795144 291.0045\n",
            "new max len 31600 20 797571 260.6348\n",
            "new max len 31700 20 800053 440.91708\n",
            "new max len 31800 20 802518 362.21338\n",
            "new max len 31900 20 805095 484.95737\n",
            "new max len 32000 20 807552 165.25902\n",
            "new max len 32100 20 809958 226.38486\n",
            "new max len 32200 20 812360 375.67294\n",
            "new max len 32300 20 814738 182.13806\n",
            "new max len 32400 20 817214 433.03815\n",
            "new max len 32500 20 819693 333.2818\n",
            "new max len 32600 20 822125 471.67252\n",
            "new max len 32700 20 824678 296.67023\n",
            "new max len 32800 20 826997 200.44025\n",
            "new max len 32900 20 829430 121.63466\n",
            "new max len 33000 20 831828 233.71281\n",
            "new max len 33100 20 834248 258.3813\n",
            "new max len 33200 20 836860 60.83801\n",
            "new max len 33300 20 839277 73.22006\n",
            "new max len 33400 20 841649 105.01203\n",
            "new max len 33500 20 844130 81.137764\n",
            "new max len 33600 20 846519 64.93318\n",
            "new max len 33700 20 848931 138.44754\n",
            "new max len 33800 20 851500 157.52768\n",
            "new max len 33900 20 854039 117.35724\n",
            "new max len 34000 20 856460 69.043846\n",
            "new max len 34100 20 858775 86.1718\n",
            "new max len 34200 20 861153 51.677525\n",
            "new max len 34300 20 863754 268.27484\n",
            "new max len 34400 20 866262 422.5916\n",
            "new max len 34500 20 868767 178.41663\n",
            "new max len 34600 20 871205 188.68939\n",
            "new max len 34700 20 873513 102.95703\n",
            "new max len 34800 20 875852 72.48004\n",
            "new max len 34900 20 878121 126.72238\n",
            "new max len 35000 20 880455 146.71806\n",
            "new max len 35100 20 882909 93.09143\n",
            "new max len 35200 20 885304 123.62608\n",
            "new max len 35300 20 887609 62.6546\n",
            "new max len 35400 20 890004 99.40958\n",
            "new max len 35500 20 892337 53.418045\n",
            "new max len 35600 20 894684 92.31999\n",
            "new max len 35700 20 897199 73.670204\n",
            "new max len 35800 20 899552 78.88451\n",
            "new max len 35900 20 901898 142.75887\n",
            "new max len 36000 20 904356 127.53163\n",
            "new max len 36100 20 906916 152.13094\n",
            "new max len 36200 20 909287 74.7827\n",
            "new max len 36300 20 911711 84.045135\n",
            "new max len 36400 20 914206 79.04884\n",
            "new max len 36500 20 916727 138.03267\n",
            "new max len 36600 20 919247 129.60799\n",
            "new max len 36700 20 921681 225.13962\n",
            "new max len 36800 20 924169 137.1123\n",
            "new max len 36900 20 926735 98.51645\n",
            "new max len 37000 20 929337 132.29718\n",
            "new max len 37100 20 931884 220.01802\n",
            "new max len 37200 20 934388 183.9687\n",
            "new max len 37300 20 936848 293.9624\n",
            "new max len 37400 20 939311 375.63983\n",
            "new max len 37500 20 941785 264.28647\n",
            "new max len 37600 20 944268 141.36227\n",
            "new max len 37700 20 946701 86.45384\n",
            "new max len 37800 20 949107 54.126743\n",
            "new max len 37900 20 951587 123.807396\n",
            "new max len 38000 20 954048 174.09607\n",
            "new max len 38100 20 956685 85.825005\n",
            "new max len 38200 20 959093 147.09758\n",
            "new max len 38300 20 961615 105.32125\n",
            "new max len 38400 20 964196 219.89447\n",
            "new max len 38500 20 966733 235.18282\n",
            "new max len 38600 20 969235 141.21161\n",
            "new max len 38700 20 971855 202.81561\n",
            "new max len 38800 20 974245 196.03061\n",
            "new max len 38900 20 976741 202.63678\n",
            "new max len 39000 20 979266 327.81885\n",
            "new max len 39100 20 981870 597.7318\n",
            "new max len 39200 20 984423 712.03613\n",
            "new max len 39300 20 987088 807.2412\n",
            "new max len 39400 20 989794 1019.54364\n",
            "new max len 39500 20 992518 1202.7914\n",
            "new max len 39600 20 995222 1229.8759\n",
            "new max len 39700 20 997941 1645.6018\n",
            "new max len 39800 20 1000523 2031.5724\n",
            "new max len 39900 20 1003268 990.46643\n",
            "new max len 40000 20 1005892 816.062\n",
            "No path found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 100  # Ширина \"луча\"\n",
        "model.eval();\n",
        "path = beam_search_sorting(permutation, beam_width=k)\n",
        "if path:\n",
        "  print(\"Кратчайший путь:\", len(path))\n",
        "  print(path)\n",
        "else:\n",
        "  print('No path found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZnF7-SAfEpQ",
        "outputId": "e9ba3cf8-73d6-4a02-b081-a3fdc1fd203d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new max len 100 100 16718 7881.874\n",
            "new max len 200 100 32815 7703.749\n",
            "new max len 300 100 48422 7548.4375\n",
            "new max len 400 100 63727 7138.293\n",
            "new max len 500 100 78555 6845.26\n",
            "new max len 600 100 93463 6316.063\n",
            "new max len 700 100 107844 4816.045\n",
            "new max len 800 100 122221 3158.1777\n",
            "new max len 900 100 137416 1685.951\n",
            "new max len 1000 100 151420 545.7935\n",
            "new max len 1100 100 165271 63.90102\n",
            "Кратчайший путь: 1136\n",
            "['R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'L', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'R', 'R', 'R', 'X', 'R', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'L', 'X', 'R', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'R', 'R', 'R', 'X', 'R', 'X', 'R', 'X', 'L', 'X', 'L', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'R', 'X', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'R', 'X', 'R', 'R', 'X', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'X', 'L', 'L', 'L', 'L', 'L', 'L']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4HPHqA-IkoW",
        "outputId": "91323144-a761-4416-f8b3-e7b90755673f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9000000, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_beam_search1(beam_width=131072, i_step_max=15000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72_tS8AEfvE9",
        "outputId": "627dcd39-5814-42bf-b5ad-7bce07a12da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40])\n",
            "torch.Size([40])\n",
            "40\n",
            "tensor(4, device='cuda:0')\n",
            "100 i_step 15.515 predict_time torch.Size([274619, 40]) array_of_states_new.shape\n",
            "tensor(7, device='cuda:0')\n",
            "200 i_step 15.015 predict_time torch.Size([264724, 40]) array_of_states_new.shape\n",
            "tensor(5, device='cuda:0')\n",
            "300 i_step 14.703 predict_time torch.Size([260730, 40]) array_of_states_new.shape\n",
            "tensor(5, device='cuda:0')\n",
            "400 i_step 15.165 predict_time torch.Size([268095, 40]) array_of_states_new.shape\n",
            "tensor(6, device='cuda:0')\n",
            "500 i_step 15.556 predict_time torch.Size([272916, 40]) array_of_states_new.shape\n",
            "tensor(6, device='cuda:0')\n",
            "600 i_step 15.629 predict_time torch.Size([276511, 40]) array_of_states_new.shape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RETsimlWltgN",
        "outputId": "e4b3b4fa-b46a-4fef-9e35-2ed5bd00011f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTqJ0-dWOTj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}